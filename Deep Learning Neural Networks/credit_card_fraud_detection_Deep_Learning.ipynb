{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "## Anonymized credit card transactions labeled as fraudulent or genuine\n",
    "\n",
    "**Context**\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "**Content**\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Identify fraudulent credit card transactions.\n",
    "\n",
    "Given the class imbalance ratio, we will use the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  StandardScaler,OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifierCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier,StackingClassifier,BaggingRegressor\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score,roc_auc_score\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation de la dataset\n",
    "dataset =pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.449044</td>\n",
       "      <td>-1.176339</td>\n",
       "      <td>0.913860</td>\n",
       "      <td>-1.375667</td>\n",
       "      <td>-1.971383</td>\n",
       "      <td>-0.629152</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-1.720408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>0.027740</td>\n",
       "      <td>0.500512</td>\n",
       "      <td>0.251367</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>-0.874300</td>\n",
       "      <td>-0.094019</td>\n",
       "      <td>2.924584</td>\n",
       "      <td>3.317027</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>-0.558895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.238422</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>-0.767315</td>\n",
       "      <td>-0.492208</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>-0.054337</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.069374</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>2.712520</td>\n",
       "      <td>-0.178398</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.096717</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036876</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.071407</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>27.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.791855</td>\n",
       "      <td>-0.327771</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>1.767473</td>\n",
       "      <td>-0.136588</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-1.907107</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151663</td>\n",
       "      <td>0.222182</td>\n",
       "      <td>1.020586</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>-0.235557</td>\n",
       "      <td>-0.164778</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>58.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.103215</td>\n",
       "      <td>-0.040296</td>\n",
       "      <td>1.267332</td>\n",
       "      <td>1.289091</td>\n",
       "      <td>-0.735997</td>\n",
       "      <td>0.288069</td>\n",
       "      <td>-0.586057</td>\n",
       "      <td>0.189380</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024612</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.382261</td>\n",
       "      <td>0.092809</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-5.401258</td>\n",
       "      <td>-5.450148</td>\n",
       "      <td>1.186305</td>\n",
       "      <td>1.736239</td>\n",
       "      <td>3.049106</td>\n",
       "      <td>-1.763406</td>\n",
       "      <td>-1.559738</td>\n",
       "      <td>0.160842</td>\n",
       "      <td>1.233090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503600</td>\n",
       "      <td>0.984460</td>\n",
       "      <td>2.458589</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>-0.481631</td>\n",
       "      <td>-0.621272</td>\n",
       "      <td>0.392053</td>\n",
       "      <td>0.949594</td>\n",
       "      <td>46.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.492936</td>\n",
       "      <td>-1.029346</td>\n",
       "      <td>0.454795</td>\n",
       "      <td>-1.438026</td>\n",
       "      <td>-1.555434</td>\n",
       "      <td>-0.720961</td>\n",
       "      <td>-1.080664</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>-1.978682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177650</td>\n",
       "      <td>-0.175074</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>-0.220385</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0    0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1    0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3    1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4    2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5    2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6    4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7    7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8    7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9    9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "10  10.0  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152   \n",
       "11  10.0  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
       "12  10.0  1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230   \n",
       "13  11.0  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544   \n",
       "14  12.0 -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596   \n",
       "15  12.0 -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850   \n",
       "16  12.0  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069   \n",
       "17  13.0 -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867   \n",
       "18  14.0 -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406   \n",
       "19  15.0  1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961   \n",
       "\n",
       "          V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0   0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2   0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3   0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4   0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "5   0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7   1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709   \n",
       "8   0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9   0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "10 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512   \n",
       "11  0.470455  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710   \n",
       "12 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285  0.084668  0.392831   \n",
       "13 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744   \n",
       "14 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182  1.020586  0.028317   \n",
       "15 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650 -0.256573 -0.065084   \n",
       "16 -0.586057  0.189380  0.782333  ... -0.024612  0.196002  0.013802  0.103758   \n",
       "17  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638 -0.156858 -0.888386   \n",
       "18 -1.559738  0.160842  1.233090  ... -0.503600  0.984460  2.458589  0.042119   \n",
       "19 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074  0.040002  0.295814   \n",
       "\n",
       "         V25       V26       V27       V28  Amount  Class  \n",
       "0   0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1   0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2  -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3   0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "6   0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7  -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8   0.373205 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9  -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "10  0.251367 -0.129478  0.042850  0.016253    7.80      0  \n",
       "11 -0.767315 -0.492208  0.042472 -0.054337    9.99      0  \n",
       "12  0.161135 -0.354990  0.026416  0.042422  121.50      0  \n",
       "13  0.548265  0.104094  0.021491  0.021293   27.50      0  \n",
       "14 -0.232746 -0.235557 -0.164778 -0.030154   58.80      0  \n",
       "15 -0.039124 -0.087086 -0.180998  0.129394   15.99      0  \n",
       "16  0.364298 -0.382261  0.092809  0.037051   12.99      0  \n",
       "17 -0.342413 -0.049027  0.079692  0.131024    0.89      0  \n",
       "18 -0.481631 -0.621272  0.392053  0.949594   46.80      0  \n",
       "19  0.332931 -0.220385  0.022298  0.007602    5.00      0  \n",
       "\n",
       "[20 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=dataset.Class==1\n",
    "datafraud=dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Time            V1            V2            V3            V4  \\\n",
      "count   284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "unique            NaN           NaN           NaN           NaN           NaN   \n",
      "top               NaN           NaN           NaN           NaN           NaN   \n",
      "freq              NaN           NaN           NaN           NaN           NaN   \n",
      "mean     94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
      "std      47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
      "min          0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
      "25%      54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
      "50%      84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
      "75%     139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
      "max     172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
      "\n",
      "                  V5            V6            V7            V8            V9  \\\n",
      "count   2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean   -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
      "std     1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
      "min    -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
      "25%    -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
      "50%    -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
      "75%     6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
      "max     3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
      "\n",
      "        ...           V22           V23           V24           V25  \\\n",
      "count   ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "unique  ...           NaN           NaN           NaN           NaN   \n",
      "top     ...           NaN           NaN           NaN           NaN   \n",
      "freq    ...           NaN           NaN           NaN           NaN   \n",
      "mean    ...  7.959909e-16  5.367590e-16  4.458112e-15  1.453003e-15   \n",
      "std     ...  7.257016e-01  6.244603e-01  6.056471e-01  5.212781e-01   \n",
      "min     ... -1.093314e+01 -4.480774e+01 -2.836627e+00 -1.029540e+01   \n",
      "25%     ... -5.423504e-01 -1.618463e-01 -3.545861e-01 -3.171451e-01   \n",
      "50%     ...  6.781943e-03 -1.119293e-02  4.097606e-02  1.659350e-02   \n",
      "75%     ...  5.285536e-01  1.476421e-01  4.395266e-01  3.507156e-01   \n",
      "max     ...  1.050309e+01  2.252841e+01  4.584549e+00  7.519589e+00   \n",
      "\n",
      "                 V26           V27           V28         Amount  \\\n",
      "count   2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
      "unique           NaN           NaN           NaN            NaN   \n",
      "top              NaN           NaN           NaN            NaN   \n",
      "freq             NaN           NaN           NaN            NaN   \n",
      "mean    1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
      "std     4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
      "min    -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
      "25%    -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
      "50%    -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
      "75%     2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
      "max     3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
      "\n",
      "                Class   time2  \n",
      "count   284807.000000  284807  \n",
      "unique            NaN      25  \n",
      "top               NaN   per21  \n",
      "freq              NaN   17703  \n",
      "mean         0.001727     NaN  \n",
      "std          0.041527     NaN  \n",
      "min          0.000000     NaN  \n",
      "25%          0.000000     NaN  \n",
      "50%          0.000000     NaN  \n",
      "75%          0.000000     NaN  \n",
      "max          1.000000     NaN  \n",
      "\n",
      "[11 rows x 32 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(284807, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic stats\n",
    "data_desc=dataset.describe(include='all')\n",
    "print(data_desc)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's divide time in differnt periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periode_per_day=24\n",
    "max_time=172792\n",
    "inter_per_day=max_time/2/periode_per_day\n",
    "inter_per_day=np.round(inter_per_day,0)\n",
    "inter_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in(x,per):\n",
    "    if x>=per[0] and x<per[1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per=[0,15]\n",
    "x=6\n",
    "get_in(x,per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(x):\n",
    "    temp1=max_time/2\n",
    "    deb=0\n",
    "    if x< temp1:\n",
    "        for i in range(periode_per_day):\n",
    "            per=[deb,deb+inter_per_day]\n",
    "            if get_in(x,per):\n",
    "                return ('per'+str(i))\n",
    "            else:\n",
    "                deb=deb+inter_per_day\n",
    "        \n",
    "    else:\n",
    "        deb=deb+inter_per_day*periode_per_day\n",
    "        for i in range(periode_per_day):\n",
    "            per=[deb,deb+inter_per_day]\n",
    "            if get_in(x,per):\n",
    "                return ('per'+str(i))\n",
    "            else:\n",
    "                deb=deb+inter_per_day\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'per23'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_period(170000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['time2']=dataset['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['time2']=dataset['time2'].apply(lambda x:str(get_period(x) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>time2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27206</th>\n",
       "      <td>34440.0</td>\n",
       "      <td>-2.604002</td>\n",
       "      <td>-0.892010</td>\n",
       "      <td>0.726497</td>\n",
       "      <td>0.615739</td>\n",
       "      <td>1.162403</td>\n",
       "      <td>-1.393118</td>\n",
       "      <td>-0.174094</td>\n",
       "      <td>0.459805</td>\n",
       "      <td>-0.408690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384888</td>\n",
       "      <td>1.854719</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>0.244255</td>\n",
       "      <td>0.167412</td>\n",
       "      <td>0.274389</td>\n",
       "      <td>-0.179089</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>per9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136223</th>\n",
       "      <td>81605.0</td>\n",
       "      <td>1.260964</td>\n",
       "      <td>0.238480</td>\n",
       "      <td>-0.160822</td>\n",
       "      <td>0.941937</td>\n",
       "      <td>0.235053</td>\n",
       "      <td>-0.109771</td>\n",
       "      <td>0.161705</td>\n",
       "      <td>-0.018062</td>\n",
       "      <td>-0.018144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005414</td>\n",
       "      <td>-0.258425</td>\n",
       "      <td>-0.491866</td>\n",
       "      <td>0.911423</td>\n",
       "      <td>-0.241012</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>-0.006090</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0</td>\n",
       "      <td>per22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137219</th>\n",
       "      <td>82069.0</td>\n",
       "      <td>-0.551364</td>\n",
       "      <td>1.015605</td>\n",
       "      <td>1.628031</td>\n",
       "      <td>-0.358399</td>\n",
       "      <td>0.184723</td>\n",
       "      <td>-1.052968</td>\n",
       "      <td>0.813188</td>\n",
       "      <td>-0.088804</td>\n",
       "      <td>-0.512041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870564</td>\n",
       "      <td>-0.147763</td>\n",
       "      <td>0.323762</td>\n",
       "      <td>0.283841</td>\n",
       "      <td>0.018915</td>\n",
       "      <td>-0.075906</td>\n",
       "      <td>-0.016066</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "      <td>per22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61083</th>\n",
       "      <td>49652.0</td>\n",
       "      <td>-0.392784</td>\n",
       "      <td>1.015525</td>\n",
       "      <td>1.380316</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>-0.110317</td>\n",
       "      <td>-0.803702</td>\n",
       "      <td>0.531996</td>\n",
       "      <td>0.124941</td>\n",
       "      <td>-0.498177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.630770</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.453890</td>\n",
       "      <td>-0.256993</td>\n",
       "      <td>0.045634</td>\n",
       "      <td>0.235708</td>\n",
       "      <td>0.087522</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>per13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68335</th>\n",
       "      <td>52943.0</td>\n",
       "      <td>-0.913753</td>\n",
       "      <td>-4.305533</td>\n",
       "      <td>-0.551163</td>\n",
       "      <td>-0.592244</td>\n",
       "      <td>-2.258949</td>\n",
       "      <td>0.386143</td>\n",
       "      <td>0.434521</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>1.961417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444254</td>\n",
       "      <td>-0.973327</td>\n",
       "      <td>-0.261625</td>\n",
       "      <td>-0.188628</td>\n",
       "      <td>-0.102891</td>\n",
       "      <td>-0.166708</td>\n",
       "      <td>0.194354</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>0</td>\n",
       "      <td>per14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40768</th>\n",
       "      <td>40392.0</td>\n",
       "      <td>0.180367</td>\n",
       "      <td>-2.320091</td>\n",
       "      <td>-2.393357</td>\n",
       "      <td>-0.099322</td>\n",
       "      <td>1.438798</td>\n",
       "      <td>3.376369</td>\n",
       "      <td>0.420557</td>\n",
       "      <td>0.468550</td>\n",
       "      <td>-1.594167</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.169199</td>\n",
       "      <td>-0.669946</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>-0.332582</td>\n",
       "      <td>-0.104583</td>\n",
       "      <td>0.122331</td>\n",
       "      <td>653.56</td>\n",
       "      <td>0</td>\n",
       "      <td>per11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162406</th>\n",
       "      <td>115087.0</td>\n",
       "      <td>2.393675</td>\n",
       "      <td>-1.101347</td>\n",
       "      <td>-1.886163</td>\n",
       "      <td>-1.589444</td>\n",
       "      <td>-0.487730</td>\n",
       "      <td>-1.038272</td>\n",
       "      <td>-0.397735</td>\n",
       "      <td>-0.496884</td>\n",
       "      <td>-1.618614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021245</td>\n",
       "      <td>0.012443</td>\n",
       "      <td>-0.632991</td>\n",
       "      <td>0.297393</td>\n",
       "      <td>-0.002429</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>-0.075047</td>\n",
       "      <td>14.95</td>\n",
       "      <td>0</td>\n",
       "      <td>per7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210115</th>\n",
       "      <td>137847.0</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.858237</td>\n",
       "      <td>0.229352</td>\n",
       "      <td>-0.618472</td>\n",
       "      <td>0.474722</td>\n",
       "      <td>-1.037323</td>\n",
       "      <td>1.018586</td>\n",
       "      <td>-0.200411</td>\n",
       "      <td>-0.039319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596417</td>\n",
       "      <td>0.056365</td>\n",
       "      <td>-0.116784</td>\n",
       "      <td>-0.467704</td>\n",
       "      <td>0.147275</td>\n",
       "      <td>0.246383</td>\n",
       "      <td>0.096147</td>\n",
       "      <td>5.38</td>\n",
       "      <td>0</td>\n",
       "      <td>per14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151756</th>\n",
       "      <td>96192.0</td>\n",
       "      <td>2.030092</td>\n",
       "      <td>-0.321602</td>\n",
       "      <td>-1.290775</td>\n",
       "      <td>0.191196</td>\n",
       "      <td>0.220023</td>\n",
       "      <td>-0.022824</td>\n",
       "      <td>-0.295418</td>\n",
       "      <td>-0.047261</td>\n",
       "      <td>2.341092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263754</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.253579</td>\n",
       "      <td>0.188381</td>\n",
       "      <td>0.132263</td>\n",
       "      <td>-0.070619</td>\n",
       "      <td>-0.076916</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0</td>\n",
       "      <td>per2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21269</th>\n",
       "      <td>31566.0</td>\n",
       "      <td>1.446566</td>\n",
       "      <td>-0.268650</td>\n",
       "      <td>-0.190397</td>\n",
       "      <td>-0.657928</td>\n",
       "      <td>-0.610225</td>\n",
       "      <td>-1.398030</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>-0.428492</td>\n",
       "      <td>-1.196663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507481</td>\n",
       "      <td>-0.169800</td>\n",
       "      <td>0.430734</td>\n",
       "      <td>0.802808</td>\n",
       "      <td>-0.085606</td>\n",
       "      <td>-0.027150</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0</td>\n",
       "      <td>per8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "27206    34440.0 -2.604002 -0.892010  0.726497  0.615739  1.162403 -1.393118   \n",
       "136223   81605.0  1.260964  0.238480 -0.160822  0.941937  0.235053 -0.109771   \n",
       "137219   82069.0 -0.551364  1.015605  1.628031 -0.358399  0.184723 -1.052968   \n",
       "61083    49652.0 -0.392784  1.015525  1.380316  0.002703 -0.110317 -0.803702   \n",
       "68335    52943.0 -0.913753 -4.305533 -0.551163 -0.592244 -2.258949  0.386143   \n",
       "40768    40392.0  0.180367 -2.320091 -2.393357 -0.099322  1.438798  3.376369   \n",
       "162406  115087.0  2.393675 -1.101347 -1.886163 -1.589444 -0.487730 -1.038272   \n",
       "210115  137847.0  0.050794  0.858237  0.229352 -0.618472  0.474722 -1.037323   \n",
       "151756   96192.0  2.030092 -0.321602 -1.290775  0.191196  0.220023 -0.022824   \n",
       "21269    31566.0  1.446566 -0.268650 -0.190397 -0.657928 -0.610225 -1.398030   \n",
       "\n",
       "              V7        V8        V9  ...       V22       V23       V24  \\\n",
       "27206  -0.174094  0.459805 -0.408690  ... -0.384888  1.854719 -0.002268   \n",
       "136223  0.161705 -0.018062 -0.018144  ... -0.005414 -0.258425 -0.491866   \n",
       "137219  0.813188 -0.088804 -0.512041  ... -0.870564 -0.147763  0.323762   \n",
       "61083   0.531996  0.124941 -0.498177  ... -0.630770  0.005123  0.453890   \n",
       "68335   0.434521  0.123100  1.961417  ... -0.444254 -0.973327 -0.261625   \n",
       "40768   0.420557  0.468550 -1.594167  ... -1.169199 -0.669946  0.998749   \n",
       "162406 -0.397735 -0.496884 -1.618614  ... -0.021245  0.012443 -0.632991   \n",
       "210115  1.018586 -0.200411 -0.039319  ... -0.596417  0.056365 -0.116784   \n",
       "151756 -0.295418 -0.047261  2.341092  ...  0.263754  0.017951  0.253579   \n",
       "21269  -0.005220 -0.428492 -1.196663  ...  0.507481 -0.169800  0.430734   \n",
       "\n",
       "             V25       V26       V27       V28   Amount  Class  time2  \n",
       "27206   0.244255  0.167412  0.274389 -0.179089     8.99      0   per9  \n",
       "136223  0.911423 -0.241012  0.003147 -0.006090     2.37      0  per22  \n",
       "137219  0.283841  0.018915 -0.075906 -0.016066     1.98      0  per22  \n",
       "61083  -0.256993  0.045634  0.235708  0.087522     0.89      0  per13  \n",
       "68335  -0.188628 -0.102891 -0.166708  0.194354  1100.00      0  per14  \n",
       "40768   0.653483 -0.332582 -0.104583  0.122331   653.56      0  per11  \n",
       "162406  0.297393 -0.002429 -0.026639 -0.075047    14.95      0   per7  \n",
       "210115 -0.467704  0.147275  0.246383  0.096147     5.38      0  per14  \n",
       "151756  0.188381  0.132263 -0.070619 -0.076916    14.50      0   per2  \n",
       "21269   0.802808 -0.085606 -0.027150  0.002715    15.00      0   per8  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00       113\n",
       "0.00        27\n",
       "99.99       27\n",
       "0.76        17\n",
       "0.77        10\n",
       "0.01         5\n",
       "2.00         4\n",
       "3.79         4\n",
       "1.10         3\n",
       "2.28         3\n",
       "12.31        3\n",
       "1.18         3\n",
       "0.68         3\n",
       "723.21       2\n",
       "188.52       2\n",
       "94.82        2\n",
       "0.83         2\n",
       "88.23        2\n",
       "7.59         2\n",
       "1.63         2\n",
       "104.03       2\n",
       "19.02        2\n",
       "39.45        2\n",
       "1.59         2\n",
       "45.51        2\n",
       "512.25       2\n",
       "105.89       2\n",
       "316.06       2\n",
       "30.31        2\n",
       "111.70       2\n",
       "18.96        2\n",
       "8.00         2\n",
       "44.90        2\n",
       "78.00        2\n",
       "101.50       2\n",
       "1.52         2\n",
       "252.92       2\n",
       "227.30       1\n",
       "261.87       1\n",
       "98.01        1\n",
       "4.90         1\n",
       "119.74       1\n",
       "529.00       1\n",
       "1354.25      1\n",
       "8.30         1\n",
       "77.89        1\n",
       "2125.87      1\n",
       "42.53        1\n",
       "112.33       1\n",
       "0.69         1\n",
       "17.39        1\n",
       "519.90       1\n",
       "105.99       1\n",
       "3.14         1\n",
       "125.30       1\n",
       "1504.93      1\n",
       "30.30        1\n",
       "130.44       1\n",
       "106.55       1\n",
       "451.27       1\n",
       "104.81       1\n",
       "11.39        1\n",
       "38.76        1\n",
       "320.01       1\n",
       "634.30       1\n",
       "118.30       1\n",
       "5.91         1\n",
       "144.80       1\n",
       "362.55       1\n",
       "139.90       1\n",
       "83.38        1\n",
       "51.37        1\n",
       "45.64        1\n",
       "22.04        1\n",
       "29.95        1\n",
       "4.56         1\n",
       "147.87       1\n",
       "188.78       1\n",
       "80.90        1\n",
       "237.26       1\n",
       "296.00       1\n",
       "776.83       1\n",
       "35.00        1\n",
       "50.00        1\n",
       "25.00        1\n",
       "40.00        1\n",
       "11.00        1\n",
       "76.94        1\n",
       "65.00        1\n",
       "33.59        1\n",
       "239.93       1\n",
       "104.00       1\n",
       "97.00        1\n",
       "173.07       1\n",
       "18.00        1\n",
       "60.00        1\n",
       "129.00       1\n",
       "320.00       1\n",
       "180.00       1\n",
       "270.00       1\n",
       "1.75         1\n",
       "5.00         1\n",
       "360.00       1\n",
       "88.00        1\n",
       "1218.89      1\n",
       "648.00       1\n",
       "59.00        1\n",
       "156.00       1\n",
       "9.21         1\n",
       "390.00       1\n",
       "204.27       1\n",
       "93.35        1\n",
       "354.33       1\n",
       "16.48        1\n",
       "99.85        1\n",
       "667.55       1\n",
       "4.87         1\n",
       "59.68        1\n",
       "549.06       1\n",
       "600.73       1\n",
       "39.98        1\n",
       "67.90        1\n",
       "996.27       1\n",
       "1809.68      1\n",
       "245.00       1\n",
       "195.66       1\n",
       "170.92       1\n",
       "459.07       1\n",
       "153.46       1\n",
       "9.13         1\n",
       "357.95       1\n",
       "4.69         1\n",
       "802.52       1\n",
       "14.46        1\n",
       "364.19       1\n",
       "9.29         1\n",
       "3.76         1\n",
       "112.45       1\n",
       "19.73        1\n",
       "124.53       1\n",
       "7.06         1\n",
       "120.54       1\n",
       "319.20       1\n",
       "60.60        1\n",
       "310.42       1\n",
       "84.28        1\n",
       "0.92         1\n",
       "5.09         1\n",
       "45.48        1\n",
       "5.30         1\n",
       "717.15       1\n",
       "99.90        1\n",
       "4.97         1\n",
       "187.11       1\n",
       "1.79         1\n",
       "349.08       1\n",
       "30.26        1\n",
       "311.28       1\n",
       "318.11       1\n",
       "53.95        1\n",
       "75.86        1\n",
       "130.21       1\n",
       "30.14        1\n",
       "9.99         1\n",
       "22.47        1\n",
       "324.59       1\n",
       "31.91        1\n",
       "2.22         1\n",
       "1335.00      1\n",
       "219.80       1\n",
       "6.27         1\n",
       "6.62         1\n",
       "4.49         1\n",
       "0.20         1\n",
       "7.52         1\n",
       "8.64         1\n",
       "273.01       1\n",
       "6.74         1\n",
       "11.40        1\n",
       "19.95        1\n",
       "11.38        1\n",
       "824.83       1\n",
       "144.62       1\n",
       "720.38       1\n",
       "106.90       1\n",
       "730.86       1\n",
       "7.18         1\n",
       "19.04        1\n",
       "80.22        1\n",
       "294.90       1\n",
       "9.82         1\n",
       "480.72       1\n",
       "1096.99      1\n",
       "340.11       1\n",
       "34.12        1\n",
       "3.90         1\n",
       "122.68       1\n",
       "7.61         1\n",
       "346.94       1\n",
       "1402.16      1\n",
       "18.98        1\n",
       "276.17       1\n",
       "247.86       1\n",
       "766.36       1\n",
       "179.66       1\n",
       "52.69        1\n",
       "6.99         1\n",
       "290.18       1\n",
       "345.00       1\n",
       "108.51       1\n",
       "727.91       1\n",
       "3.22         1\n",
       "635.10       1\n",
       "172.32       1\n",
       "311.91       1\n",
       "3.12         1\n",
       "33.76        1\n",
       "571.48       1\n",
       "113.92       1\n",
       "19.59        1\n",
       "454.82       1\n",
       "127.14       1\n",
       "37.32        1\n",
       "45.49        1\n",
       "45.03        1\n",
       "0.38         1\n",
       "238.90       1\n",
       "39.90        1\n",
       "489.71       1\n",
       "8.90         1\n",
       "444.17       1\n",
       "829.41       1\n",
       "254.76       1\n",
       "10.70        1\n",
       "720.80       1\n",
       "8.54         1\n",
       "7.58         1\n",
       "37.93        1\n",
       "426.40       1\n",
       "24.90        1\n",
       "30.39        1\n",
       "7.57         1\n",
       "23.36        1\n",
       "240.77       1\n",
       "209.65       1\n",
       "261.22       1\n",
       "1389.56      1\n",
       "7.53         1\n",
       "208.58       1\n",
       "592.90       1\n",
       "2.27         1\n",
       "186.13       1\n",
       "17.06        1\n",
       "925.31       1\n",
       "323.77       1\n",
       "57.73        1\n",
       "3.93         1\n",
       "2.47         1\n",
       "175.90       1\n",
       "Name: Amount, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.set_option('display.max_rows', None)\n",
    "dataset[dataset.Class==1]['Amount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***we have 492 frauds out of 284,807 transactions (0.172%)***\n",
    "\n",
    "***27 frauds have an amount of 27***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class', 'time2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=dataset.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'time2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=col.drop(['Class'])\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = col\n",
    "target_variable = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables explicatives :  Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'time2'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = dataset.loc[:, features_list]\n",
    "Y = dataset.loc[:, target_variable]\n",
    "\n",
    "print('Variables explicatives : ', X.columns)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']  at positions  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Found categorical features  ['time2']  at positions  [30]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "numeric_features = []\n",
    "numeric_indices = []\n",
    "categorical_features = []\n",
    "categorical_indices = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "  if ('float' in str(t)) or ('int' in str(t)) :\n",
    "    numeric_features.append(i)\n",
    "    numeric_indices.append(idx)\n",
    "  else :\n",
    "    categorical_features.append(i)\n",
    "    categorical_indices.append(idx)\n",
    "\n",
    "  idx = idx + 1\n",
    "\n",
    "print('Found numeric features ', numeric_features,' at positions ', numeric_indices)\n",
    "print('Found categorical features ', categorical_features,' at positions ', categorical_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42,stratify=Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert pandas DataFrames to numpy arrays...\n",
      "...Done\n",
      "[[161919.0 1.9467466672816798 -0.752525821492348 -1.3551295328913102\n",
      "  -0.661629913230695 1.50282190770174 4.02493282673061\n",
      "  -1.4796614838390902 1.1398802168957198 1.4068187137481298\n",
      "  -0.15740291266068202 -0.11372919032959901 0.510277474082635\n",
      "  0.0612577807353701 -0.0665551394957695 1.3287018205003602\n",
      "  0.352513902823239 -0.76566982664618 0.14193753685938199\n",
      "  -0.451364651514137 -0.134434695487417 0.0761965101986691\n",
      "  0.297536541043559 0.30791456949789503 0.690980284177926\n",
      "  -0.350316231256655 -0.388907264903717 0.0776408382302655\n",
      "  -0.0322477642202041 7.32 'per20']\n",
      " [124477.0 2.03514918623719 -0.0488803143003422 -3.0586934537638997\n",
      "  0.24794503371213297 2.94348685227235 3.2986973115731204\n",
      "  -0.0021924639358902698 0.674781823977897 0.0458257088345251\n",
      "  0.28486427695311 -0.254903244909153 0.325559776048587\n",
      "  -0.405326546442527 0.7210684077145 -0.14844514500485 -0.754029381288908\n",
      "  -0.270842357681763 -0.6956978417796721 -0.274411134595354\n",
      "  -0.227279116186558 0.0386282061449729 0.22819701963924\n",
      "  0.0355424132400545 0.707089987721862 0.512884596281977\n",
      "  -0.4711976248462921 0.00251988539144107 -0.0690016835527196 2.99\n",
      "  'per10']\n",
      " [41191.0 -0.991919644382192 0.603192643431104 0.7119760179456609\n",
      "  -0.992424654946317 -0.8258377052649359 1.9562613478420299\n",
      "  -2.21260332725614 -5.0375230501797095 0.0007717251978341111\n",
      "  -2.00956091808815 -0.38684511730923704 1.82016085004487\n",
      "  0.74777695874234 0.12274616958530599 -1.7232846037194398\n",
      "  1.12334415094773 -0.7246159945795899 0.147254864861436\n",
      "  0.004631177665532889 1.2808563137731799 -2.79835228768859\n",
      "  0.109525862489931 -0.436530155014404 -0.9328030395609179\n",
      "  0.826684291046232 0.913773193731371 0.0380486856951838\n",
      "  0.18534031963132003 175.1 'per11']\n",
      " [132624.0 2.28571782188968 -1.5002392770555701 -0.747565164139995\n",
      "  -1.66811860629904 -1.3941425936346599 -0.35033876190345303\n",
      "  -1.42798360644237 0.010009593384969 -1.11844672075498\n",
      "  1.7561210638799998 0.0931364463331114 -0.7224498786977801\n",
      "  -0.46875724226235205 -0.195287757066107 -0.6386832156829301\n",
      "  -0.0658875748672835 0.0727807169728085 0.7682372951547\n",
      "  0.257424115538122 -0.490641541573627 -0.13966991990313699\n",
      "  0.0770132158391883 0.208310169754412 -0.538236349525744\n",
      "  -0.27803203024071305 -0.16206818757852698 0.0180452709860015\n",
      "  -0.0630052317493315 6.1 'per12']\n",
      " [59359.0 -0.44874695349004 -1.01144001580821 0.11590267005546401\n",
      "  -3.45485413841999 0.715770533817782 -0.14749000524300798\n",
      "  0.5043474049271861 -0.11381684311720901 -0.0447824514547183\n",
      "  -0.558955013780937 -0.251076432441015 -0.0547083000860354\n",
      "  -0.782698209036227 0.13465916603516198 -0.48300743189908707\n",
      "  -2.09609949871472 -0.39952522499015897 1.59758878430156\n",
      "  -0.08274558161131099 -0.275296971138216 -0.243245217795912\n",
      "  -0.173298449581609 -0.0066924613429996 -1.3623830528357\n",
      "  -0.29223427597027896 -0.144621719580633 -0.032580422581582\n",
      "  -0.0641935650630091 86.1 'per16']]\n",
      "[[160760.0 -0.6744660645783139 1.40810501967799 -1.11062205357093\n",
      "  -1.32836577843066 1.3889960325483701 -1.30843906707795\n",
      "  1.8858789026871698 -0.614232966299775 0.31165221245310104\n",
      "  0.65075700363522 -0.8577846615478051 -0.229961445775592\n",
      "  -0.19981700479102998 0.26637132632987903 -0.0465441684754424\n",
      "  -0.741398089749789 -0.605616644106022 -0.39256818789208003\n",
      "  -0.162648311024695 0.394321820843914 0.0800842396026648\n",
      "  0.810033595602455 -0.22432723043641198 0.7078992374468671\n",
      "  -0.13583702273753 0.0451021964988772 0.533837219064273\n",
      "  0.291319252625364 23.0 'per20']\n",
      " [19847.0 -2.8298159432727905 -2.7651492122926897 2.5377929506399703\n",
      "  -1.07458042259191 2.84255889292598 -2.15353644367205\n",
      "  -1.7955188572036898 -0.250020372324704 3.07350426338358\n",
      "  -1.00041794567463 1.85084176033081 -1.54977887088034 1.25233673695303\n",
      "  0.963974373307274 -0.48102722379162605 -0.14731917203129\n",
      "  -0.20932817158106604 1.05889790297841 0.397056508186649\n",
      "  -0.515764693656115 -0.295554564490287 0.10930496669883599\n",
      "  -0.8132719687442499 0.0429955726118499 -0.0276596849389052\n",
      "  -0.9102470549045358 0.11080175906673699 -0.511938134872381 11.85 'per5']]\n",
      "\n",
      "[0 0 0 0 0]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "print(\"...Done\")\n",
    "\n",
    "print(X_train[0:5,:])\n",
    "print(X_test[0:2,:])\n",
    "print()\n",
    "print(Y_train[0:5])\n",
    "print(Y_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features and standardizing numerical features...\n",
      "...Done\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.41158751e+00  9.93379083e-01 -4.56036586e-01 -8.94051557e-01\n",
      "  -4.67283725e-01  1.08921729e+00  3.02438347e+00 -1.19485202e+00\n",
      "   9.57057418e-01  1.28137638e+00 -1.44546401e-01 -1.10814578e-01\n",
      "   5.09338566e-01  6.21173504e-02 -6.95576832e-02  1.45109663e+00\n",
      "   4.04446404e-01 -9.07403516e-01  1.69133453e-01 -5.55365736e-01\n",
      "  -1.73112388e-01  1.03171150e-01  4.09563109e-01  4.90684977e-01\n",
      "   1.14196982e+00 -6.70831980e-01 -8.07226946e-01  1.91911522e-01\n",
      "  -9.91057622e-02 -3.22493763e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   6.23140848e-01  1.03850725e+00 -2.93491180e-02 -2.01830171e+00\n",
      "   1.75133177e-01  2.13350569e+00  2.47884001e+00 -1.83232506e-03\n",
      "   5.66703699e-01  4.11208797e-02  2.62604208e-01 -2.49329490e-01\n",
      "   3.23973351e-01 -4.07235825e-01  7.54324041e-01 -1.63023227e-01\n",
      "  -8.61271777e-01 -3.21276918e-01 -8.29392525e-01 -3.37959105e-01\n",
      "  -2.93563756e-01  5.20550968e-02  3.14019107e-01  5.65684612e-02\n",
      "   1.16858333e+00  9.85237737e-01 -9.77975788e-01  6.39700564e-03\n",
      "  -2.11524226e-01 -3.39763883e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -1.13068022e+00 -5.06766132e-01  3.66064988e-01  4.70114301e-01\n",
      "  -7.00918274e-01 -5.98747859e-01  1.47041074e+00 -1.78668444e+00\n",
      "  -4.22759248e+00  6.37611546e-05 -1.84964069e+00 -3.78786117e-01\n",
      "   1.82381366e+00  7.52710535e-01  1.28458094e-01 -1.88389441e+00\n",
      "   1.28615964e+00 -8.58774965e-01  1.75472118e-01  4.87470958e-03\n",
      "   1.66301019e+00 -3.80798730e+00  1.50500280e-01 -6.95837737e-01\n",
      "  -1.54054919e+00  1.58726933e+00  1.89577715e+00  9.41369499e-02\n",
      "   5.66426473e-01  3.46693487e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   7.94698856e-01  1.16641879e+00 -9.09446660e-01 -4.93095462e-01\n",
      "  -1.17814911e+00 -1.01069250e+00 -2.62292439e-01 -1.15312341e+00\n",
      "   8.76517232e-03 -1.01986568e+00  1.61704132e+00  9.21545550e-02\n",
      "  -7.27710041e-01 -4.71042938e-01 -2.04216495e-01 -6.98720096e-01\n",
      "  -7.41416141e-02  8.57470164e-02  9.15731071e-01  3.15458314e-01\n",
      "  -6.35236029e-01 -1.90540271e-01  1.05700600e-01  3.31931911e-01\n",
      "  -8.88717980e-01 -5.32153155e-01 -3.36545934e-01  4.47376323e-02\n",
      "  -1.93183001e-01 -3.27359709e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -7.48101861e-01 -2.29484595e-01 -6.13040988e-01  7.67415923e-02\n",
      "  -2.44008922e+00  5.18710857e-01 -1.09913757e-01  4.07186093e-01\n",
      "  -9.51614606e-02 -4.14491768e-02 -5.14214752e-01 -2.45574759e-01\n",
      "  -5.76277330e-02 -7.86846922e-01  1.40919504e-01 -5.28608807e-01\n",
      "  -2.39639704e+00 -4.73702675e-01  1.90438201e+00 -1.02477138e-01\n",
      "  -3.55859543e-01 -3.31466528e-01 -2.39207725e-01 -1.07469974e-02\n",
      "  -2.25022303e+00 -5.59400475e-01 -3.00345288e-01 -8.02847579e-02\n",
      "  -1.96817732e-01 -8.28127279e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Put here all the preprocessings\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "\n",
    "if categorical_indices==[]:\n",
    "    # Normalization\n",
    "    print(\"just scaling\")\n",
    "    featureencoder = StandardScaler()\n",
    "    \n",
    "elif numeric_indices==[]:\n",
    "    # OHE / dummyfication\n",
    "    print(\"encoding\")\n",
    "    featureencoder = OneHotEncoder(drop='first') \n",
    "    \n",
    "else:\n",
    "\n",
    "    # Normalization\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    # OHE / dummyfication\n",
    "    categorical_transformer = OneHotEncoder(drop='first')\n",
    "    featureencoder = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_indices),    \n",
    "            ('num', numeric_transformer, numeric_indices)\n",
    "            ]    )\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((54,), ()), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "full_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=X_train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 54), (None,)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=5000\n",
    "full_ds =full_ds.batch(BATCH_SIZE)\n",
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Dense(32, activation=\"relu\", input_shape=[input_shape]),\n",
    "                             tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# Création d'un compileur\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss= tf.keras.losses.binary_crossentropy,\n",
    "              metrics =METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 46 steps\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0041 - tp: 282.0000 - fp: 32.0000 - tn: 227419.0000 - fn: 112.0000 - accuracy: 0.9994 - precision: 0.8981 - recall: 0.7157 - auc: 0.9289\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0040 - tp: 286.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 108.0000 - accuracy: 0.9994 - precision: 0.8966 - recall: 0.7259 - auc: 0.9307\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0038 - tp: 289.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 105.0000 - accuracy: 0.9994 - precision: 0.8975 - recall: 0.7335 - auc: 0.9323\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0037 - tp: 292.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 102.0000 - accuracy: 0.9994 - precision: 0.8985 - recall: 0.7411 - auc: 0.9325\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0036 - tp: 293.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 101.0000 - accuracy: 0.9994 - precision: 0.8988 - recall: 0.7437 - auc: 0.9352\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0035 - tp: 295.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 99.0000 - accuracy: 0.9994 - precision: 0.8994 - recall: 0.7487 - auc: 0.9340\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0034 - tp: 296.0000 - fp: 33.0000 - tn: 227418.0000 - fn: 98.0000 - accuracy: 0.9994 - precision: 0.8997 - recall: 0.7513 - auc: 0.9341\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0033 - tp: 296.0000 - fp: 34.0000 - tn: 227417.0000 - fn: 98.0000 - accuracy: 0.9994 - precision: 0.8970 - recall: 0.7513 - auc: 0.9367\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0032 - tp: 299.0000 - fp: 35.0000 - tn: 227416.0000 - fn: 95.0000 - accuracy: 0.9994 - precision: 0.8952 - recall: 0.7589 - auc: 0.9368\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0032 - tp: 298.0000 - fp: 35.0000 - tn: 227416.0000 - fn: 96.0000 - accuracy: 0.9994 - precision: 0.8949 - recall: 0.7563 - auc: 0.9369\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0031 - tp: 298.0000 - fp: 35.0000 - tn: 227416.0000 - fn: 96.0000 - accuracy: 0.9994 - precision: 0.8949 - recall: 0.7563 - auc: 0.9369\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0030 - tp: 299.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 95.0000 - accuracy: 0.9994 - precision: 0.8925 - recall: 0.7589 - auc: 0.9382\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0030 - tp: 300.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 94.0000 - accuracy: 0.9994 - precision: 0.8929 - recall: 0.7614 - auc: 0.9395\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0029 - tp: 300.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 94.0000 - accuracy: 0.9994 - precision: 0.8929 - recall: 0.7614 - auc: 0.9408\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0029 - tp: 300.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 94.0000 - accuracy: 0.9994 - precision: 0.8929 - recall: 0.7614 - auc: 0.9420\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0028 - tp: 300.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 94.0000 - accuracy: 0.9994 - precision: 0.8929 - recall: 0.7614 - auc: 0.9446\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0028 - tp: 301.0000 - fp: 37.0000 - tn: 227414.0000 - fn: 93.0000 - accuracy: 0.9994 - precision: 0.8905 - recall: 0.7640 - auc: 0.9484\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0027 - tp: 301.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 93.0000 - accuracy: 0.9994 - precision: 0.8932 - recall: 0.7640 - auc: 0.9484\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0027 - tp: 304.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 90.0000 - accuracy: 0.9994 - precision: 0.8941 - recall: 0.7716 - auc: 0.9485\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0027 - tp: 303.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 91.0000 - accuracy: 0.9994 - precision: 0.8938 - recall: 0.7690 - auc: 0.9497\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0026 - tp: 303.0000 - fp: 36.0000 - tn: 227415.0000 - fn: 91.0000 - accuracy: 0.9994 - precision: 0.8938 - recall: 0.7690 - auc: 0.9510\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0026 - tp: 303.0000 - fp: 34.0000 - tn: 227417.0000 - fn: 91.0000 - accuracy: 0.9995 - precision: 0.8991 - recall: 0.7690 - auc: 0.9511\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0026 - tp: 304.0000 - fp: 32.0000 - tn: 227419.0000 - fn: 90.0000 - accuracy: 0.9995 - precision: 0.9048 - recall: 0.7716 - auc: 0.9524\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - tp: 304.0000 - fp: 30.0000 - tn: 227421.0000 - fn: 90.0000 - accuracy: 0.9995 - precision: 0.9102 - recall: 0.7716 - auc: 0.9537\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0025 - tp: 304.0000 - fp: 28.0000 - tn: 227423.0000 - fn: 90.0000 - accuracy: 0.9995 - precision: 0.9157 - recall: 0.7716 - auc: 0.9549\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0024 - tp: 303.0000 - fp: 25.0000 - tn: 227426.0000 - fn: 91.0000 - accuracy: 0.9995 - precision: 0.9238 - recall: 0.7690 - auc: 0.9562\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0024 - tp: 303.0000 - fp: 24.0000 - tn: 227427.0000 - fn: 91.0000 - accuracy: 0.9995 - precision: 0.9266 - recall: 0.7690 - auc: 0.9562\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0024 - tp: 304.0000 - fp: 23.0000 - tn: 227428.0000 - fn: 90.0000 - accuracy: 0.9995 - precision: 0.9297 - recall: 0.7716 - auc: 0.9563\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0024 - tp: 304.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 90.0000 - accuracy: 0.9995 - precision: 0.9383 - recall: 0.7716 - auc: 0.9563 0s - loss: 0.0023 - tp: 164.0000 - fp: 10.0000 - tn: 124777.0000 - fn: 49.0000 - accuracy: 0.9995 - precision: 0.9425 - recall: 0.7700\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0023 - tp: 306.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 88.0000 - accuracy: 0.9995 - precision: 0.9387 - recall: 0.7766 - auc: 0.9563\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0023 - tp: 305.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 89.0000 - accuracy: 0.9995 - precision: 0.9385 - recall: 0.7741 - auc: 0.9563\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0023 - tp: 305.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 89.0000 - accuracy: 0.9995 - precision: 0.9385 - recall: 0.7741 - auc: 0.9576\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0022 - tp: 305.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 89.0000 - accuracy: 0.9995 - precision: 0.9385 - recall: 0.7741 - auc: 0.9576\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0022 - tp: 306.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 88.0000 - accuracy: 0.9995 - precision: 0.9387 - recall: 0.7766 - auc: 0.9576\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0022 - tp: 305.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 89.0000 - accuracy: 0.9995 - precision: 0.9385 - recall: 0.7741 - auc: 0.9576\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0021 - tp: 305.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 89.0000 - accuracy: 0.9995 - precision: 0.9414 - recall: 0.7741 - auc: 0.9589s - loss: 0.0025 - tp: 73.0000 - fp: 4.0000 - tn: 54899.0000 - fn: 24.0000 - accuracy: 0.9995 - precision: 0.9481 - recall:\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0021 - tp: 306.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 88.0000 - accuracy: 0.9995 - precision: 0.9415 - recall: 0.7766 - auc: 0.9589\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0021 - tp: 308.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 86.0000 - accuracy: 0.9995 - precision: 0.9390 - recall: 0.7817 - auc: 0.9589\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0021 - tp: 309.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 85.0000 - accuracy: 0.9995 - precision: 0.9392 - recall: 0.7843 - auc: 0.9589\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0020 - tp: 310.0000 - fp: 20.0000 - tn: 227431.0000 - fn: 84.0000 - accuracy: 0.9995 - precision: 0.9394 - recall: 0.7868 - auc: 0.9602\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0020 - tp: 309.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 85.0000 - accuracy: 0.9995 - precision: 0.9421 - recall: 0.7843 - auc: 0.9615\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0020 - tp: 309.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 85.0000 - accuracy: 0.9995 - precision: 0.9421 - recall: 0.7843 - auc: 0.9615\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0020 - tp: 311.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 83.0000 - accuracy: 0.9996 - precision: 0.9424 - recall: 0.7893 - auc: 0.9628\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0019 - tp: 312.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 82.0000 - accuracy: 0.9996 - precision: 0.9426 - recall: 0.7919 - auc: 0.9640\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0019 - tp: 315.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 79.0000 - accuracy: 0.9996 - precision: 0.9431 - recall: 0.7995 - auc: 0.9640\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0019 - tp: 316.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 78.0000 - accuracy: 0.9996 - precision: 0.9433 - recall: 0.8020 - auc: 0.9641\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0019 - tp: 316.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 78.0000 - accuracy: 0.9996 - precision: 0.9433 - recall: 0.8020 - auc: 0.9641\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0018 - tp: 316.0000 - fp: 19.0000 - tn: 227432.0000 - fn: 78.0000 - accuracy: 0.9996 - precision: 0.9433 - recall: 0.8020 - auc: 0.9641\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0018 - tp: 317.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 77.0000 - accuracy: 0.9996 - precision: 0.9463 - recall: 0.8046 - auc: 0.9666\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0018 - tp: 317.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 77.0000 - accuracy: 0.9996 - precision: 0.9463 - recall: 0.8046 - auc: 0.9666\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0018 - tp: 318.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 76.0000 - accuracy: 0.9996 - precision: 0.9464 - recall: 0.8071 - auc: 0.9666\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0017 - tp: 319.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 75.0000 - accuracy: 0.9996 - precision: 0.9466 - recall: 0.8096 - auc: 0.9679\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0017 - tp: 320.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 74.0000 - accuracy: 0.9996 - precision: 0.9467 - recall: 0.8122 - auc: 0.9692\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0017 - tp: 322.0000 - fp: 18.0000 - tn: 227433.0000 - fn: 72.0000 - accuracy: 0.9996 - precision: 0.9471 - recall: 0.8173 - auc: 0.9704\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0017 - tp: 322.0000 - fp: 16.0000 - tn: 227435.0000 - fn: 72.0000 - accuracy: 0.9996 - precision: 0.9527 - recall: 0.8173 - auc: 0.9704\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0016 - tp: 323.0000 - fp: 15.0000 - tn: 227436.0000 - fn: 71.0000 - accuracy: 0.9996 - precision: 0.9556 - recall: 0.8198 - auc: 0.9704\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0016 - tp: 326.0000 - fp: 15.0000 - tn: 227436.0000 - fn: 68.0000 - accuracy: 0.9996 - precision: 0.9560 - recall: 0.8274 - auc: 0.9705\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0016 - tp: 325.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 69.0000 - accuracy: 0.9996 - precision: 0.9615 - recall: 0.8249 - auc: 0.9705\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0016 - tp: 325.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 69.0000 - accuracy: 0.9996 - precision: 0.9615 - recall: 0.8249 - auc: 0.9705s - loss: 0.0015 - tp: 154.0000 - fp: 7.0000 - tn: 114805.0000 - fn: 34.0000 - accuracy: 0.9996 - precision: 0.9565 - recall: 0.8191 - a\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0016 - tp: 326.0000 - fp: 14.0000 - tn: 227437.0000 - fn: 68.0000 - accuracy: 0.9996 - precision: 0.9588 - recall: 0.8274 - auc: 0.9718\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0015 - tp: 327.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 67.0000 - accuracy: 0.9996 - precision: 0.9618 - recall: 0.8299 - auc: 0.9743\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0015 - tp: 328.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 66.0000 - accuracy: 0.9997 - precision: 0.9619 - recall: 0.8325 - auc: 0.9756\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0015 - tp: 331.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 63.0000 - accuracy: 0.9997 - precision: 0.9622 - recall: 0.8401 - auc: 0.9756\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0015 - tp: 333.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 61.0000 - accuracy: 0.9997 - precision: 0.9624 - recall: 0.8452 - auc: 0.9756\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0014 - tp: 332.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 62.0000 - accuracy: 0.9997 - precision: 0.9623 - recall: 0.8426 - auc: 0.9756\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0014 - tp: 333.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 61.0000 - accuracy: 0.9997 - precision: 0.9624 - recall: 0.8452 - auc: 0.9781\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0014 - tp: 332.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 62.0000 - accuracy: 0.9997 - precision: 0.9623 - recall: 0.8426 - auc: 0.9782s - loss: 0.0014 - tp: 121.0000 - fp: 5.0000 - tn: 89851.0000 - fn: 23.0000 - accuracy: 0.9997 - precision: 0.9603 - recall: 0.8403\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0014 - tp: 332.0000 - fp: 14.0000 - tn: 227437.0000 - fn: 62.0000 - accuracy: 0.9997 - precision: 0.9595 - recall: 0.8426 - auc: 0.9794\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0014 - tp: 333.0000 - fp: 13.0000 - tn: 227438.0000 - fn: 61.0000 - accuracy: 0.9997 - precision: 0.9624 - recall: 0.8452 - auc: 0.9794\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0013 - tp: 336.0000 - fp: 12.0000 - tn: 227439.0000 - fn: 58.0000 - accuracy: 0.9997 - precision: 0.9655 - recall: 0.8528 - auc: 0.9795\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0013 - tp: 341.0000 - fp: 11.0000 - tn: 227440.0000 - fn: 53.0000 - accuracy: 0.9997 - precision: 0.9688 - recall: 0.8655 - auc: 0.9807\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0013 - tp: 343.0000 - fp: 10.0000 - tn: 227441.0000 - fn: 51.0000 - accuracy: 0.9997 - precision: 0.9717 - recall: 0.8706 - auc: 0.9820\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0013 - tp: 343.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 51.0000 - accuracy: 0.9997 - precision: 0.9744 - recall: 0.8706 - auc: 0.9833\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0012 - tp: 343.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 51.0000 - accuracy: 0.9997 - precision: 0.9744 - recall: 0.8706 - auc: 0.9846\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0012 - tp: 343.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 51.0000 - accuracy: 0.9997 - precision: 0.9744 - recall: 0.8706 - auc: 0.9846\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0012 - tp: 344.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 50.0000 - accuracy: 0.9997 - precision: 0.9745 - recall: 0.8731 - auc: 0.9846\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0012 - tp: 344.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 50.0000 - accuracy: 0.9997 - precision: 0.9773 - recall: 0.8731 - auc: 0.9846\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0012 - tp: 345.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 49.0000 - accuracy: 0.9997 - precision: 0.9773 - recall: 0.8756 - auc: 0.9846\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0011 - tp: 344.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 50.0000 - accuracy: 0.9997 - precision: 0.9745 - recall: 0.8731 - auc: 0.9846\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0011 - tp: 346.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 48.0000 - accuracy: 0.9998 - precision: 0.9774 - recall: 0.8782 - auc: 0.9846\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0011 - tp: 345.0000 - fp: 9.0000 - tn: 227442.0000 - fn: 49.0000 - accuracy: 0.9997 - precision: 0.9746 - recall: 0.8756 - auc: 0.9846\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0011 - tp: 346.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 48.0000 - accuracy: 0.9998 - precision: 0.9774 - recall: 0.8782 - auc: 0.9846\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0011 - tp: 348.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 46.0000 - accuracy: 0.9998 - precision: 0.9775 - recall: 0.8832 - auc: 0.9846\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0010 - tp: 348.0000 - fp: 7.0000 - tn: 227444.0000 - fn: 46.0000 - accuracy: 0.9998 - precision: 0.9803 - recall: 0.8832 - auc: 0.9846\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0010 - tp: 346.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 48.0000 - accuracy: 0.9998 - precision: 0.9774 - recall: 0.8782 - auc: 0.9872\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0010 - tp: 349.0000 - fp: 7.0000 - tn: 227444.0000 - fn: 45.0000 - accuracy: 0.9998 - precision: 0.9803 - recall: 0.8858 - auc: 0.9872\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.9401e-04 - tp: 350.0000 - fp: 8.0000 - tn: 227443.0000 - fn: 44.0000 - accuracy: 0.9998 - precision: 0.9777 - recall: 0.8883 - auc: 0.9872\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.7741e-04 - tp: 351.0000 - fp: 5.0000 - tn: 227446.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9860 - recall: 0.8909 - auc: 0.9872\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.6051e-04 - tp: 349.0000 - fp: 7.0000 - tn: 227444.0000 - fn: 45.0000 - accuracy: 0.9998 - precision: 0.9803 - recall: 0.8858 - auc: 0.9872\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.4602e-04 - tp: 352.0000 - fp: 5.0000 - tn: 227446.0000 - fn: 42.0000 - accuracy: 0.9998 - precision: 0.9860 - recall: 0.8934 - auc: 0.9872\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.2866e-04 - tp: 351.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9832 - recall: 0.8909 - auc: 0.9885\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 9.0811e-04 - tp: 353.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 41.0000 - accuracy: 0.9998 - precision: 0.9833 - recall: 0.8959 - auc: 0.9885\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 8.9021e-04 - tp: 356.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9834 - recall: 0.9036 - auc: 0.9885\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 8.7362e-04 - tp: 356.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9834 - recall: 0.9036 - auc: 0.9885\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 8.5459e-04 - tp: 358.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9835 - recall: 0.9086 - auc: 0.9897\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 8.3911e-04 - tp: 356.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9834 - recall: 0.9036 - auc: 0.9897\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 8.1999e-04 - tp: 357.0000 - fp: 5.0000 - tn: 227446.0000 - fn: 37.0000 - accuracy: 0.9998 - precision: 0.9862 - recall: 0.9061 - auc: 0.9898\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 8.0545e-04 - tp: 360.0000 - fp: 4.0000 - tn: 227447.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9890 - recall: 0.9137 - auc: 0.9898\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 7.9151e-04 - tp: 360.0000 - fp: 5.0000 - tn: 227446.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9863 - recall: 0.9137 - auc: 0.9923\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 7.7631e-04 - tp: 360.0000 - fp: 3.0000 - tn: 227448.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9917 - recall: 0.9137 - auc: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20215cf5688>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(full_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features and standardizing numerical features...\n",
      "...Done\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   1.3871815  -0.34471083  0.85416023 -0.73269126 -0.93818762  1.00670883\n",
      "  -0.98201123  1.52273861 -0.51515709  0.28336517  0.59944458 -0.84085508\n",
      "  -0.23349522 -0.20050673  0.27869502 -0.05167318 -0.84682349 -0.7178194\n",
      "  -0.4680386  -0.20064633  0.51286793  0.10846086  1.11574075 -0.3576211\n",
      "   1.16992023 -0.25934891  0.09332341  1.31850867  0.89058207 -0.25995439]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "  -1.58013819 -1.44498471 -1.67648235  1.67504412 -0.75894346  2.06034649\n",
      "  -1.61684295 -1.44989904 -0.2094761   2.80020559 -0.92062496  1.81675188\n",
      "  -1.55793919  1.26026452  1.00841216 -0.52644498 -0.16728705 -0.24841293\n",
      "   1.26222093  0.48701189 -0.66782946 -0.40263949  0.1501959  -1.29630236\n",
      "   0.07148752 -0.05180832 -1.88898386  0.27380388 -1.56632492 -0.30442595]\n",
      " [ 1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.13812017 -1.82615312  1.40617205  0.86278481  2.30508347  0.81739017\n",
      "   2.15323428  1.16603298 -0.6030217   1.70715436  6.81139967  2.04271776\n",
      "  -0.06709726  0.58147932 -2.54201419  1.64482797 -1.41479336 -0.46289931\n",
      "  -1.46847272  0.12049573  2.64111756 -1.44296259  0.02282454 -0.21055943\n",
      "  -2.45112943 -0.56664608  0.13009236  1.36437772  1.55873319 -0.04828573]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.98653554  1.05139028 -0.009036   -0.71416173  0.27265222 -0.01776351\n",
      "  -0.80660452  0.16772531 -0.28343449  0.41407945  0.04441778 -0.63943394\n",
      "   0.75073352  0.67004864  0.16597071  0.02907126 -0.19457216 -0.34542444\n",
      "  -1.20231351  0.11840867 -0.24782618 -0.38376689 -0.88149179  0.52878346\n",
      "  -0.11119118 -0.54297938  0.42205122 -0.15694161 -0.18422669 -0.34774084]\n",
      " [ 0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -1.18227186  0.6172629   0.83972705 -0.88639722  1.24563931  0.47998921\n",
      "  -1.58668039  0.68955342 -0.39910851 -0.57443881 -1.45386906  1.43579626\n",
      "   0.20682779  0.73939226 -3.70146377  1.01113646  0.95637126  3.37057019\n",
      "   1.24082266 -1.28503415  0.01307993 -0.22366453 -0.45277837 -0.24653716\n",
      "   1.02379932  1.57252339 -0.68608694  0.11595503  0.31924358 -0.34570672]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "\n",
    "X_test = featureencoder.transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_test[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.0032043995977047183\n",
      "tp :  81.0\n",
      "fp :  15.0\n",
      "tn :  56849.0\n",
      "fn :  17.0\n",
      "accuracy :  0.9994382\n",
      "precision :  0.84375\n",
      "recall :  0.82653064\n",
      "auc :  0.94355744\n",
      "\n",
      "Legitimate Transactions Detected (True Negatives):  56849\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  15\n",
      "Fraudulent Transactions Missed (False Negatives):  17\n",
      "Fraudulent Transactions Detected (True Positives):  81\n",
      "Total Fraudulent Transactions:  98\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_results = model.evaluate(X_test,Y_test,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "\n",
    "plot_cm(Y_test, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "## Oversample the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pd=pd.DataFrame(X_train)\n",
    "Y_train_pd=pd.DataFrame(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_train_labels = Y_train != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227845"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = X_train_pd[bool_train_labels]\n",
    "neg_features = X_train_pd[~bool_train_labels]\n",
    "\n",
    "pos_labels = Y_train_pd[bool_train_labels]\n",
    "neg_labels =Y_train_pd[~bool_train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb 0: 227451\n",
      "nb 1: 394\n"
     ]
    }
   ],
   "source": [
    "print(\"nb 0:\",len(neg_features))\n",
    "print(\"nb 1:\",len(pos_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choices shape 227451\n"
     ]
    }
   ],
   "source": [
    "ids = np.arange(len(pos_features))\n",
    "choices = np.random.choice(ids, len(neg_features))\n",
    "\n",
    "print(\"choices shape\",len(choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 75,  73,  45, ...,  63, 118, 205])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911114</td>\n",
       "      <td>-0.065896</td>\n",
       "      <td>-0.826271</td>\n",
       "      <td>0.114983</td>\n",
       "      <td>-0.991333</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>-0.769968</td>\n",
       "      <td>0.029735</td>\n",
       "      <td>0.445777</td>\n",
       "      <td>-0.347701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767273</td>\n",
       "      <td>0.388404</td>\n",
       "      <td>-1.063490</td>\n",
       "      <td>-0.422765</td>\n",
       "      <td>-1.441881</td>\n",
       "      <td>1.804239</td>\n",
       "      <td>-0.454853</td>\n",
       "      <td>2.159775</td>\n",
       "      <td>1.438437</td>\n",
       "      <td>-0.347701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064825</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>-0.375906</td>\n",
       "      <td>-0.050365</td>\n",
       "      <td>-0.670535</td>\n",
       "      <td>0.303941</td>\n",
       "      <td>-0.216872</td>\n",
       "      <td>0.182415</td>\n",
       "      <td>-0.127620</td>\n",
       "      <td>-0.347701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.357689</td>\n",
       "      <td>0.139521</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>-6.581340</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>-2.173639</td>\n",
       "      <td>0.564677</td>\n",
       "      <td>4.547364</td>\n",
       "      <td>-5.494968</td>\n",
       "      <td>2.521532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.450028</td>\n",
       "      <td>7.569015</td>\n",
       "      <td>-2.216480</td>\n",
       "      <td>1.538484</td>\n",
       "      <td>0.270923</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>0.967010</td>\n",
       "      <td>0.688058</td>\n",
       "      <td>4.501870</td>\n",
       "      <td>0.070651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.116992</td>\n",
       "      <td>2.407391</td>\n",
       "      <td>-2.331813</td>\n",
       "      <td>-1.666711</td>\n",
       "      <td>0.237335</td>\n",
       "      <td>3.093103</td>\n",
       "      <td>-0.460021</td>\n",
       "      <td>3.658143</td>\n",
       "      <td>1.339613</td>\n",
       "      <td>0.047119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.041956</td>\n",
       "      <td>2.376224</td>\n",
       "      <td>-1.864942</td>\n",
       "      <td>-1.216121</td>\n",
       "      <td>0.193790</td>\n",
       "      <td>2.491487</td>\n",
       "      <td>-0.466764</td>\n",
       "      <td>4.003435</td>\n",
       "      <td>1.481810</td>\n",
       "      <td>0.047119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505065</td>\n",
       "      <td>0.283632</td>\n",
       "      <td>-0.705561</td>\n",
       "      <td>-0.930583</td>\n",
       "      <td>-0.362730</td>\n",
       "      <td>2.830603</td>\n",
       "      <td>1.018940</td>\n",
       "      <td>1.281542</td>\n",
       "      <td>1.230734</td>\n",
       "      <td>-0.347701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425233</td>\n",
       "      <td>0.407367</td>\n",
       "      <td>-0.804132</td>\n",
       "      <td>-0.299237</td>\n",
       "      <td>-0.543478</td>\n",
       "      <td>1.406245</td>\n",
       "      <td>0.120251</td>\n",
       "      <td>1.366186</td>\n",
       "      <td>0.974736</td>\n",
       "      <td>-0.344710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038766</td>\n",
       "      <td>-0.146081</td>\n",
       "      <td>-0.344965</td>\n",
       "      <td>-0.831467</td>\n",
       "      <td>-0.741215</td>\n",
       "      <td>2.479309</td>\n",
       "      <td>1.071093</td>\n",
       "      <td>0.022761</td>\n",
       "      <td>0.468479</td>\n",
       "      <td>-0.348977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227451 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9   ...        44  \\\n",
       "387  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.911114   \n",
       "279  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.767273   \n",
       "112  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.064825   \n",
       "151  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ... -2.357689   \n",
       "19   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -1.450028   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "36   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.116992   \n",
       "195  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.041956   \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.505065   \n",
       "344  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.425233   \n",
       "334  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.038766   \n",
       "\n",
       "           45        46        47        48        49        50        51  \\\n",
       "387 -0.065896 -0.826271  0.114983 -0.991333  0.713663 -0.769968  0.029735   \n",
       "279  0.388404 -1.063490 -0.422765 -1.441881  1.804239 -0.454853  2.159775   \n",
       "112  0.236380 -0.375906 -0.050365 -0.670535  0.303941 -0.216872  0.182415   \n",
       "151  0.139521  0.428977 -6.581340  0.057682 -2.173639  0.564677  4.547364   \n",
       "19   7.569015 -2.216480  1.538484  0.270923  0.092446  0.967010  0.688058   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "36   2.407391 -2.331813 -1.666711  0.237335  3.093103 -0.460021  3.658143   \n",
       "195  2.376224 -1.864942 -1.216121  0.193790  2.491487 -0.466764  4.003435   \n",
       "1    0.283632 -0.705561 -0.930583 -0.362730  2.830603  1.018940  1.281542   \n",
       "344  0.407367 -0.804132 -0.299237 -0.543478  1.406245  0.120251  1.366186   \n",
       "334 -0.146081 -0.344965 -0.831467 -0.741215  2.479309  1.071093  0.022761   \n",
       "\n",
       "           52        53  \n",
       "387  0.445777 -0.347701  \n",
       "279  1.438437 -0.347701  \n",
       "112 -0.127620 -0.347701  \n",
       "151 -5.494968  2.521532  \n",
       "19   4.501870  0.070651  \n",
       "..        ...       ...  \n",
       "36   1.339613  0.047119  \n",
       "195  1.481810  0.047119  \n",
       "1    1.230734 -0.347701  \n",
       "344  0.974736 -0.344710  \n",
       "334  0.468479 -0.348977  \n",
       "\n",
       "[227451 rows x 54 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1f= pos_features.reset_index()\n",
    "temp2f=temp1f.drop('index', axis=1)\n",
    "temp2f.iloc[choices,:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227451 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "387  1\n",
       "279  1\n",
       "112  1\n",
       "151  1\n",
       "19   1\n",
       "..  ..\n",
       "36   1\n",
       "195  1\n",
       "1    1\n",
       "344  1\n",
       "334  1\n",
       "\n",
       "[227451 rows x 1 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1l=pos_labels.reset_index()\n",
    "temp2l=temp1l.drop('index', axis=1)\n",
    "temp2l.iloc[choices,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pos_features = temp2f.iloc[choices,:]\n",
    "res_pos_labels = temp2l.iloc[choices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227451, 54)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pos_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227451, 1)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pos_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454902, 54)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "resampled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((54,), (1,)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds_resampled = tf.data.Dataset.from_tensor_slices((resampled_features, resampled_labels))\n",
    "full_ds_resampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 54), (None, 1)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=50\n",
    "full_ds_resampled =full_ds_resampled.batch(BATCH_SIZE)\n",
    "full_ds_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Dense(32, activation=\"relu\", input_shape=[input_shape]),\n",
    "                             tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# Création d'un compileur\n",
    "model2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss= tf.keras.losses.binary_crossentropy,\n",
    "              metrics =METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 9099 steps\n",
      "Epoch 1/10\n",
      "9099/9099 [==============================] - 14s 2ms/step - loss: 2.2475e-04 - tp: 227448.0000 - fp: 5.0000 - tn: 227446.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 2/10\n",
      "9099/9099 [==============================] - 14s 2ms/step - loss: 9.0037e-05 - tp: 227446.0000 - fp: 4.0000 - tn: 227447.0000 - fn: 5.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 3/10\n",
      "9099/9099 [==============================] - 14s 2ms/step - loss: 9.9825e-05 - tp: 227447.0000 - fp: 4.0000 - tn: 227447.0000 - fn: 4.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 4/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 8.8754e-05 - tp: 227448.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 5/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 1.2092e-04 - tp: 227446.0000 - fp: 4.0000 - tn: 227447.0000 - fn: 5.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 6/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 3.8979e-05 - tp: 227450.0000 - fp: 3.0000 - tn: 227448.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 7/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 1.5530e-05 - tp: 227450.0000 - fp: 1.0000 - tn: 227450.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 8/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 2.6983e-04 - tp: 227445.0000 - fp: 7.0000 - tn: 227444.0000 - fn: 6.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 9/10\n",
      "9099/9099 [==============================] - 14s 2ms/step - loss: 7.1881e-06 - tp: 227451.0000 - fp: 1.0000 - tn: 227450.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000: 2s - loss: 8.7684e-06 - tp: 185922.0000 - fp: 1.0000 - tn: 185977.0000 - fn: 0.0000e+00 - accuracy: 1.000\n",
      "Epoch 10/10\n",
      "9099/9099 [==============================] - 15s 2ms/step - loss: 3.1787e-04 - tp: 227448.0000 - fp: 6.0000 - tn: 227445.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20206ce7bc8>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(full_ds_resampled, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_resampled = model2.predict(resampled_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_resampled  = model2.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.07980899731700507\n",
      "tp :  73.0\n",
      "fp :  19.0\n",
      "tn :  56845.0\n",
      "fn :  25.0\n",
      "accuracy :  0.9992276\n",
      "precision :  0.79347825\n",
      "recall :  0.74489796\n",
      "auc :  0.8824857\n",
      "\n",
      "Legitimate Transactions Detected (True Negatives):  56845\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  19\n",
      "Fraudulent Transactions Missed (False Negatives):  25\n",
      "Fraudulent Transactions Detected (True Positives):  73\n",
      "Total Fraudulent Transactions:  98\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_results = model2.evaluate(X_test,Y_test,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model2.metrics_names, sampled_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "\n",
    "plot_cm(Y_test, test_predictions_resampled )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights\n",
    "## Calculate class weights\n",
    "The goal is to identify fradulent transactions, but we don't have very many of those positive samples to work with, so we would want to have the classifier heavily weight the few examples that are available. \n",
    "\n",
    "We can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.50\n",
      "Weight for class 1: 289.14\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "\n",
    "total=len(neg_features)+len(pos_features)\n",
    "\n",
    "weight_for_0 = (1 /len(neg_features))*(total)/2.0 \n",
    "weight_for_1 = (1 /len(pos_features))*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance des classes\n",
    "class_weight = {\n",
    "    0:0.5,\n",
    "    1:495,\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 495}"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Dense(32, activation=\"relu\", input_shape=[input_shape]),\n",
    "                             tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss= tf.keras.losses.binary_crossentropy,\n",
    "              metrics =METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 46 steps\n",
      "Epoch 1/110\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 1.1227 - tp: 239.0000 - fp: 58383.0000 - tn: 169068.0000 - fn: 155.0000 - accuracy: 0.7431 - precision: 0.0041 - recall: 0.6066 - auc: 0.6650\n",
      "Epoch 2/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.4430 - tp: 366.0000 - fp: 59413.0000 - tn: 168038.0000 - fn: 28.0000 - accuracy: 0.7391 - precision: 0.0061 - recall: 0.9289 - auc: 0.9460\n",
      "Epoch 3/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.3448 - tp: 361.0000 - fp: 17603.0000 - tn: 209848.0000 - fn: 33.0000 - accuracy: 0.9226 - precision: 0.0201 - recall: 0.9162 - auc: 0.9580\n",
      "Epoch 4/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.2783 - tp: 363.0000 - fp: 9110.0000 - tn: 218341.0000 - fn: 31.0000 - accuracy: 0.9599 - precision: 0.0383 - recall: 0.9213 - auc: 0.9716\n",
      "Epoch 5/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.2323 - tp: 368.0000 - fp: 7868.0000 - tn: 219583.0000 - fn: 26.0000 - accuracy: 0.9654 - precision: 0.0447 - recall: 0.9340 - auc: 0.9807\n",
      "Epoch 6/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.2009 - tp: 370.0000 - fp: 7522.0000 - tn: 219929.0000 - fn: 24.0000 - accuracy: 0.9669 - precision: 0.0469 - recall: 0.9391 - auc: 0.9859\n",
      "Epoch 7/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.1784 - tp: 372.0000 - fp: 7572.0000 - tn: 219879.0000 - fn: 22.0000 - accuracy: 0.9667 - precision: 0.0468 - recall: 0.9442 - auc: 0.9886\n",
      "Epoch 8/110\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.1614 - tp: 373.0000 - fp: 7611.0000 - tn: 219840.0000 - fn: 21.0000 - accuracy: 0.9665 - precision: 0.0467 - recall: 0.9467 - auc: 0.9906\n",
      "Epoch 9/110\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.1475 - tp: 376.0000 - fp: 7454.0000 - tn: 219997.0000 - fn: 18.0000 - accuracy: 0.9672 - precision: 0.0480 - recall: 0.9543 - auc: 0.9921\n",
      "Epoch 10/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.1365 - tp: 379.0000 - fp: 7309.0000 - tn: 220142.0000 - fn: 15.0000 - accuracy: 0.9679 - precision: 0.0493 - recall: 0.9619 - auc: 0.9932 0s - loss: 0.1437 - tp: 302.0000 - fp: 5943.0000 - tn: 178740.0000 - fn: 15.0000 - accuracy: 0.9678 - precision: 0.0484 - recall: 0.9527 - auc: \n",
      "Epoch 11/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.1266 - tp: 381.0000 - fp: 6952.0000 - tn: 220499.0000 - fn: 13.0000 - accuracy: 0.9694 - precision: 0.0520 - recall: 0.9670 - auc: 0.9943\n",
      "Epoch 12/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.1185 - tp: 385.0000 - fp: 6768.0000 - tn: 220683.0000 - fn: 9.0000 - accuracy: 0.9703 - precision: 0.0538 - recall: 0.9772 - auc: 0.9951\n",
      "Epoch 13/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.1105 - tp: 385.0000 - fp: 6408.0000 - tn: 221043.0000 - fn: 9.0000 - accuracy: 0.9718 - precision: 0.0567 - recall: 0.9772 - auc: 0.9958\n",
      "Epoch 14/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.1036 - tp: 385.0000 - fp: 6161.0000 - tn: 221290.0000 - fn: 9.0000 - accuracy: 0.9729 - precision: 0.0588 - recall: 0.9772 - auc: 0.9964\n",
      "Epoch 15/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0970 - tp: 387.0000 - fp: 5826.0000 - tn: 221625.0000 - fn: 7.0000 - accuracy: 0.9744 - precision: 0.0623 - recall: 0.9822 - auc: 0.9968\n",
      "Epoch 16/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0908 - tp: 388.0000 - fp: 5611.0000 - tn: 221840.0000 - fn: 6.0000 - accuracy: 0.9753 - precision: 0.0647 - recall: 0.9848 - auc: 0.9972\n",
      "Epoch 17/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0845 - tp: 389.0000 - fp: 5349.0000 - tn: 222102.0000 - fn: 5.0000 - accuracy: 0.9765 - precision: 0.0678 - recall: 0.9873 - auc: 0.9976\n",
      "Epoch 18/110\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.0782 - tp: 389.0000 - fp: 5116.0000 - tn: 222335.0000 - fn: 5.0000 - accuracy: 0.9775 - precision: 0.0707 - recall: 0.9873 - auc: 0.9980\n",
      "Epoch 19/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0727 - tp: 389.0000 - fp: 4900.0000 - tn: 222551.0000 - fn: 5.0000 - accuracy: 0.9785 - precision: 0.0735 - recall: 0.9873 - auc: 0.9982\n",
      "Epoch 20/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0676 - tp: 390.0000 - fp: 4756.0000 - tn: 222695.0000 - fn: 4.0000 - accuracy: 0.9791 - precision: 0.0758 - recall: 0.9898 - auc: 0.9984\n",
      "Epoch 21/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0627 - tp: 390.0000 - fp: 4483.0000 - tn: 222968.0000 - fn: 4.0000 - accuracy: 0.9803 - precision: 0.0800 - recall: 0.9898 - auc: 0.9986\n",
      "Epoch 22/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0585 - tp: 391.0000 - fp: 4455.0000 - tn: 222996.0000 - fn: 3.0000 - accuracy: 0.9804 - precision: 0.0807 - recall: 0.9924 - auc: 0.9988\n",
      "Epoch 23/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0544 - tp: 392.0000 - fp: 4210.0000 - tn: 223241.0000 - fn: 2.0000 - accuracy: 0.9815 - precision: 0.0852 - recall: 0.9949 - auc: 0.9989\n",
      "Epoch 24/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0503 - tp: 392.0000 - fp: 4115.0000 - tn: 223336.0000 - fn: 2.0000 - accuracy: 0.9819 - precision: 0.0870 - recall: 0.9949 - auc: 0.9990\n",
      "Epoch 25/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0468 - tp: 392.0000 - fp: 3898.0000 - tn: 223553.0000 - fn: 2.0000 - accuracy: 0.9829 - precision: 0.0914 - recall: 0.9949 - auc: 0.9991\n",
      "Epoch 26/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0435 - tp: 393.0000 - fp: 3726.0000 - tn: 223725.0000 - fn: 1.0000 - accuracy: 0.9836 - precision: 0.0954 - recall: 0.9975 - auc: 0.9991\n",
      "Epoch 27/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0409 - tp: 393.0000 - fp: 3537.0000 - tn: 223914.0000 - fn: 1.0000 - accuracy: 0.9845 - precision: 0.1000 - recall: 0.9975 - auc: 0.9992\n",
      "Epoch 28/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0386 - tp: 393.0000 - fp: 3416.0000 - tn: 224035.0000 - fn: 1.0000 - accuracy: 0.9850 - precision: 0.1032 - recall: 0.9975 - auc: 0.9992\n",
      "Epoch 29/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0364 - tp: 393.0000 - fp: 3203.0000 - tn: 224248.0000 - fn: 1.0000 - accuracy: 0.9859 - precision: 0.1093 - recall: 0.9975 - auc: 0.9993\n",
      "Epoch 30/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0347 - tp: 393.0000 - fp: 3123.0000 - tn: 224328.0000 - fn: 1.0000 - accuracy: 0.9863 - precision: 0.1118 - recall: 0.9975 - auc: 0.9993\n",
      "Epoch 31/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0328 - tp: 394.0000 - fp: 2977.0000 - tn: 224474.0000 - fn: 0.0000e+00 - accuracy: 0.9869 - precision: 0.1169 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 32/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0313 - tp: 394.0000 - fp: 2855.0000 - tn: 224596.0000 - fn: 0.0000e+00 - accuracy: 0.9875 - precision: 0.1213 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 33/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0297 - tp: 394.0000 - fp: 2718.0000 - tn: 224733.0000 - fn: 0.0000e+00 - accuracy: 0.9881 - precision: 0.1266 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 34/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0284 - tp: 394.0000 - fp: 2612.0000 - tn: 224839.0000 - fn: 0.0000e+00 - accuracy: 0.9885 - precision: 0.1311 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 35/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0271 - tp: 394.0000 - fp: 2538.0000 - tn: 224913.0000 - fn: 0.0000e+00 - accuracy: 0.9889 - precision: 0.1344 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 36/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0259 - tp: 394.0000 - fp: 2423.0000 - tn: 225028.0000 - fn: 0.0000e+00 - accuracy: 0.9894 - precision: 0.1399 - recall: 1.0000 - auc: 0.9994\n",
      "Epoch 37/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0248 - tp: 394.0000 - fp: 2342.0000 - tn: 225109.0000 - fn: 0.0000e+00 - accuracy: 0.9897 - precision: 0.1440 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 38/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0237 - tp: 394.0000 - fp: 2264.0000 - tn: 225187.0000 - fn: 0.0000e+00 - accuracy: 0.9901 - precision: 0.1482 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 39/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0230 - tp: 394.0000 - fp: 2200.0000 - tn: 225251.0000 - fn: 0.0000e+00 - accuracy: 0.9903 - precision: 0.1519 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 40/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0220 - tp: 394.0000 - fp: 2081.0000 - tn: 225370.0000 - fn: 0.0000e+00 - accuracy: 0.9909 - precision: 0.1592 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 41/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0211 - tp: 394.0000 - fp: 2019.0000 - tn: 225432.0000 - fn: 0.0000e+00 - accuracy: 0.9911 - precision: 0.1633 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 42/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0204 - tp: 394.0000 - fp: 1966.0000 - tn: 225485.0000 - fn: 0.0000e+00 - accuracy: 0.9914 - precision: 0.1669 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 43/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0196 - tp: 394.0000 - fp: 1887.0000 - tn: 225564.0000 - fn: 0.0000e+00 - accuracy: 0.9917 - precision: 0.1727 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 44/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0189 - tp: 394.0000 - fp: 1808.0000 - tn: 225643.0000 - fn: 0.0000e+00 - accuracy: 0.9921 - precision: 0.1789 - recall: 1.0000 - auc: 0.9995\n",
      "Epoch 45/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0183 - tp: 394.0000 - fp: 1735.0000 - tn: 225716.0000 - fn: 0.0000e+00 - accuracy: 0.9924 - precision: 0.1851 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 46/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0176 - tp: 394.0000 - fp: 1669.0000 - tn: 225782.0000 - fn: 0.0000e+00 - accuracy: 0.9927 - precision: 0.1910 - recall: 1.0000 - auc: 0.99960s - loss: 0.0180 - tp: 220.0000 - fp: 886.0000 - tn: 128894.0000 - fn: 0.0000e+00 - accuracy: 0.9932 - precision: 0.1989 - recall: 1.0000 - auc\n",
      "Epoch 47/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0171 - tp: 394.0000 - fp: 1625.0000 - tn: 225826.0000 - fn: 0.0000e+00 - accuracy: 0.9929 - precision: 0.1951 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 48/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0164 - tp: 394.0000 - fp: 1547.0000 - tn: 225904.0000 - fn: 0.0000e+00 - accuracy: 0.9932 - precision: 0.2030 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 49/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0160 - tp: 394.0000 - fp: 1519.0000 - tn: 225932.0000 - fn: 0.0000e+00 - accuracy: 0.9933 - precision: 0.2060 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 50/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0155 - tp: 394.0000 - fp: 1463.0000 - tn: 225988.0000 - fn: 0.0000e+00 - accuracy: 0.9936 - precision: 0.2122 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 51/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0151 - tp: 394.0000 - fp: 1424.0000 - tn: 226027.0000 - fn: 0.0000e+00 - accuracy: 0.9938 - precision: 0.2167 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 52/110\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.0147 - tp: 394.0000 - fp: 1413.0000 - tn: 226038.0000 - fn: 0.0000e+00 - accuracy: 0.9938 - precision: 0.2180 - recall: 1.0000 - auc: 0.9996\n",
      "Epoch 53/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0139 - tp: 394.0000 - fp: 1312.0000 - tn: 226139.0000 - fn: 0.0000e+00 - accuracy: 0.9942 - precision: 0.2309 - recall: 1.0000 - auc: 0.9996: 0s - loss: 0.0141 - tp: 380.0000 - fp: 1270.0000 - tn: 218350.0000 - fn: 0.0000e+00 - accuracy: 0.9942 - precision: 0.2303 - recall: 1.0000 - auc: 0.999\n",
      "Epoch 54/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0135 - tp: 394.0000 - fp: 1298.0000 - tn: 226153.0000 - fn: 0.0000e+00 - accuracy: 0.9943 - precision: 0.2329 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 55/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0130 - tp: 394.0000 - fp: 1219.0000 - tn: 226232.0000 - fn: 0.0000e+00 - accuracy: 0.9946 - precision: 0.2443 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 56/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0127 - tp: 394.0000 - fp: 1201.0000 - tn: 226250.0000 - fn: 0.0000e+00 - accuracy: 0.9947 - precision: 0.2470 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 57/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0122 - tp: 394.0000 - fp: 1156.0000 - tn: 226295.0000 - fn: 0.0000e+00 - accuracy: 0.9949 - precision: 0.2542 - recall: 1.0000 - auc: 0.99970s - loss: 0.0120 - tp: 188.0000 - fp: 554.0000 - tn: 114258.0000 - fn: 0.0000e+00 - accuracy: 0.9952 - precision: 0.2534 - recall: 1.0000 -\n",
      "Epoch 58/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0118 - tp: 394.0000 - fp: 1117.0000 - tn: 226334.0000 - fn: 0.0000e+00 - accuracy: 0.9951 - precision: 0.2608 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 59/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0116 - tp: 394.0000 - fp: 1090.0000 - tn: 226361.0000 - fn: 0.0000e+00 - accuracy: 0.9952 - precision: 0.2655 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 60/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0112 - tp: 394.0000 - fp: 1053.0000 - tn: 226398.0000 - fn: 0.0000e+00 - accuracy: 0.9954 - precision: 0.2723 - recall: 1.0000 - auc: 0.9997 - loss: 0.0104 - tp: 105.0000 - fp: 282.0000 - tn: 59613.0000 - fn: 0.0000e+00 - accuracy: 0.9953 - precision: 0.2713 - recall: 1.00\n",
      "Epoch 61/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0110 - tp: 394.0000 - fp: 1057.0000 - tn: 226394.0000 - fn: 0.0000e+00 - accuracy: 0.9954 - precision: 0.2715 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 62/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0106 - tp: 394.0000 - fp: 997.0000 - tn: 226454.0000 - fn: 0.0000e+00 - accuracy: 0.9956 - precision: 0.2832 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 63/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0102 - tp: 394.0000 - fp: 967.0000 - tn: 226484.0000 - fn: 0.0000e+00 - accuracy: 0.9958 - precision: 0.2895 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 64/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0099 - tp: 394.0000 - fp: 929.0000 - tn: 226522.0000 - fn: 0.0000e+00 - accuracy: 0.9959 - precision: 0.2978 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 65/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0096 - tp: 394.0000 - fp: 893.0000 - tn: 226558.0000 - fn: 0.0000e+00 - accuracy: 0.9961 - precision: 0.3061 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 66/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0094 - tp: 394.0000 - fp: 904.0000 - tn: 226547.0000 - fn: 0.0000e+00 - accuracy: 0.9960 - precision: 0.3035 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 67/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0091 - tp: 394.0000 - fp: 846.0000 - tn: 226605.0000 - fn: 0.0000e+00 - accuracy: 0.9963 - precision: 0.3177 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 68/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0088 - tp: 394.0000 - fp: 844.0000 - tn: 226607.0000 - fn: 0.0000e+00 - accuracy: 0.9963 - precision: 0.3183 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 69/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0085 - tp: 394.0000 - fp: 812.0000 - tn: 226639.0000 - fn: 0.0000e+00 - accuracy: 0.9964 - precision: 0.3267 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 70/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0082 - tp: 394.0000 - fp: 782.0000 - tn: 226669.0000 - fn: 0.0000e+00 - accuracy: 0.9966 - precision: 0.3350 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 71/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0080 - tp: 394.0000 - fp: 761.0000 - tn: 226690.0000 - fn: 0.0000e+00 - accuracy: 0.9967 - precision: 0.3411 - recall: 1.0000 - auc: 0.9997\n",
      "Epoch 72/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0078 - tp: 394.0000 - fp: 725.0000 - tn: 226726.0000 - fn: 0.0000e+00 - accuracy: 0.9968 - precision: 0.3521 - recall: 1.0000 - auc: 0.9997 0s - loss: 0.0076 - tp: 317.0000 - fp: 572.0000 - tn: 184111.0000 - fn: 0.0000e+00 - accuracy: 0.9969 - precision: 0.3566 - recall: 1.0000 - auc: 0.\n",
      "Epoch 73/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0076 - tp: 394.0000 - fp: 725.0000 - tn: 226726.0000 - fn: 0.0000e+00 - accuracy: 0.9968 - precision: 0.3521 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 74/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0074 - tp: 394.0000 - fp: 688.0000 - tn: 226763.0000 - fn: 0.0000e+00 - accuracy: 0.9970 - precision: 0.3641 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 75/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0071 - tp: 394.0000 - fp: 674.0000 - tn: 226777.0000 - fn: 0.0000e+00 - accuracy: 0.9970 - precision: 0.3689 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 76/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0070 - tp: 394.0000 - fp: 656.0000 - tn: 226795.0000 - fn: 0.0000e+00 - accuracy: 0.9971 - precision: 0.3752 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 77/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0067 - tp: 394.0000 - fp: 626.0000 - tn: 226825.0000 - fn: 0.0000e+00 - accuracy: 0.9973 - precision: 0.3863 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 78/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0067 - tp: 394.0000 - fp: 630.0000 - tn: 226821.0000 - fn: 0.0000e+00 - accuracy: 0.9972 - precision: 0.3848 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 79/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0064 - tp: 394.0000 - fp: 596.0000 - tn: 226855.0000 - fn: 0.0000e+00 - accuracy: 0.9974 - precision: 0.3980 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 80/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0062 - tp: 394.0000 - fp: 580.0000 - tn: 226871.0000 - fn: 0.0000e+00 - accuracy: 0.9975 - precision: 0.4045 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 81/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0060 - tp: 394.0000 - fp: 552.0000 - tn: 226899.0000 - fn: 0.0000e+00 - accuracy: 0.9976 - precision: 0.4165 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 82/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0059 - tp: 394.0000 - fp: 552.0000 - tn: 226899.0000 - fn: 0.0000e+00 - accuracy: 0.9976 - precision: 0.4165 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 83/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0057 - tp: 394.0000 - fp: 513.0000 - tn: 226938.0000 - fn: 0.0000e+00 - accuracy: 0.9977 - precision: 0.4344 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 84/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0055 - tp: 394.0000 - fp: 495.0000 - tn: 226956.0000 - fn: 0.0000e+00 - accuracy: 0.9978 - precision: 0.4432 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 85/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0055 - tp: 394.0000 - fp: 497.0000 - tn: 226954.0000 - fn: 0.0000e+00 - accuracy: 0.9978 - precision: 0.4422 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 86/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0053 - tp: 394.0000 - fp: 467.0000 - tn: 226984.0000 - fn: 0.0000e+00 - accuracy: 0.9980 - precision: 0.4576 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 87/110\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.0052 - tp: 394.0000 - fp: 446.0000 - tn: 227005.0000 - fn: 0.0000e+00 - accuracy: 0.9980 - precision: 0.4690 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 88/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0050 - tp: 394.0000 - fp: 432.0000 - tn: 227019.0000 - fn: 0.0000e+00 - accuracy: 0.9981 - precision: 0.4770 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 89/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0049 - tp: 394.0000 - fp: 427.0000 - tn: 227024.0000 - fn: 0.0000e+00 - accuracy: 0.9981 - precision: 0.4799 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 90/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0047 - tp: 394.0000 - fp: 400.0000 - tn: 227051.0000 - fn: 0.0000e+00 - accuracy: 0.9982 - precision: 0.4962 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 91/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0046 - tp: 394.0000 - fp: 390.0000 - tn: 227061.0000 - fn: 0.0000e+00 - accuracy: 0.9983 - precision: 0.5026 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 92/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0045 - tp: 394.0000 - fp: 377.0000 - tn: 227074.0000 - fn: 0.0000e+00 - accuracy: 0.9983 - precision: 0.5110 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 93/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0044 - tp: 394.0000 - fp: 380.0000 - tn: 227071.0000 - fn: 0.0000e+00 - accuracy: 0.9983 - precision: 0.5090 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 94/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0043 - tp: 394.0000 - fp: 344.0000 - tn: 227107.0000 - fn: 0.0000e+00 - accuracy: 0.9985 - precision: 0.5339 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 95/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0042 - tp: 394.0000 - fp: 346.0000 - tn: 227105.0000 - fn: 0.0000e+00 - accuracy: 0.9985 - precision: 0.5324 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 96/110\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.0040 - tp: 394.0000 - fp: 331.0000 - tn: 227120.0000 - fn: 0.0000e+00 - accuracy: 0.9985 - precision: 0.5434 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 97/110\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0039 - tp: 394.0000 - fp: 328.0000 - tn: 227123.0000 - fn: 0.0000e+00 - accuracy: 0.9986 - precision: 0.5457 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 98/110\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0038 - tp: 394.0000 - fp: 309.0000 - tn: 227142.0000 - fn: 0.0000e+00 - accuracy: 0.9986 - precision: 0.5605 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 99/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0038 - tp: 394.0000 - fp: 312.0000 - tn: 227139.0000 - fn: 0.0000e+00 - accuracy: 0.9986 - precision: 0.5581 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 100/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0037 - tp: 394.0000 - fp: 298.0000 - tn: 227153.0000 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.5694 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 101/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0036 - tp: 394.0000 - fp: 288.0000 - tn: 227163.0000 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.5777 - recall: 1.0000 - auc: 0.9998\n",
      "Epoch 102/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0035 - tp: 394.0000 - fp: 286.0000 - tn: 227165.0000 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.5794 - recall: 1.0000 - auc: 0.9998 0s - loss: 0.0035 - tp: 188.0000 - fp: 148.0000 - tn: 114664.0000 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.5595 - recall: 1.0000\n",
      "Epoch 103/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0034 - tp: 394.0000 - fp: 278.0000 - tn: 227173.0000 - fn: 0.0000e+00 - accuracy: 0.9988 - precision: 0.5863 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 104/110\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0033 - tp: 394.0000 - fp: 264.0000 - tn: 227187.0000 - fn: 0.0000e+00 - accuracy: 0.9988 - precision: 0.5988 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 105/110\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.0033 - tp: 394.0000 - fp: 267.0000 - tn: 227184.0000 - fn: 0.0000e+00 - accuracy: 0.9988 - precision: 0.5961 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 106/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0032 - tp: 394.0000 - fp: 254.0000 - tn: 227197.0000 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.6080 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 107/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0031 - tp: 394.0000 - fp: 260.0000 - tn: 227191.0000 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.6024 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 108/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 20ms/step - loss: 0.0030 - tp: 394.0000 - fp: 240.0000 - tn: 227211.0000 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.6215 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 109/110\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.0030 - tp: 394.0000 - fp: 253.0000 - tn: 227198.0000 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.6090 - recall: 1.0000 - auc: 0.9999\n",
      "Epoch 110/110\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 0.0029 - tp: 394.0000 - fp: 244.0000 - tn: 227207.0000 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.6176 - recall: 1.0000 - auc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x202f9acb8c8>"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(full_ds,  epochs=110, \n",
    "     \n",
    "                    class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_w = model3.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_w = model3.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.011168410957483763\n",
      "tp :  86.0\n",
      "fp :  103.0\n",
      "tn :  56761.0\n",
      "fn :  12.0\n",
      "accuracy :  0.99798113\n",
      "precision :  0.45502645\n",
      "recall :  0.877551\n",
      "auc :  0.9533219\n",
      "\n",
      "Legitimate Transactions Detected (True Negatives):  56761\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  103\n",
      "Fraudulent Transactions Missed (False Negatives):  12\n",
      "Fraudulent Transactions Detected (True Positives):  86\n",
      "Total Fraudulent Transactions:  98\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAFNCAYAAABvx4bHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debxVdb3/8dcbEDxqCkoggjkkVmaF2lXKWQzRMmywnNG8kl79qWWWmjfHSm+DN69m4Yg4WxpoKhHlVA6AokimIE4MisogqChwPr8/1vfg5niGvTZ7n322+/3ksR5nr+/6rrW++xzO53y+67vWdysiMDOz4nWpdgPMzGqNA6eZWU4OnGZmOTlwmpnl5MBpZpaTA6eZWU4OnGZmOTlwdkKSGiTdIWmxpFvX4DiHSvpLOdtWLZJ2lfRMtdthBg6ca0TSIZImS1oqaZ6kuyXtUoZDfxPoC2wUEQeWepCIuD4ihpahPRUlKSRt1VadiHggIj6xhucZmv4gvSJpvqQHJR0tqUuzehtKul3SW5JelHRIG8c8W9Ly9H+gadmyYPsgSVMkvZ2+DlqT92CdgwNniSR9H/hf4GdkQe5jwG+B4WU4/GbAsxGxogzHqnmSupXhGP9D9rO6Avgk0A84AdgTuFNSj4LqlwLvkf1cDwUuk/TpNg5/c0SsV7DMSufsDowFrgN6AaOBsancallEeMm5ABsAS4ED26jTgyywzk3L/wI90rY9gNnAKcB8YB5wVNp2Dtkv7fJ0jqOBs4HrCo69ORBAt7R+JDALWAI8DxxaUP5gwX5fBCYBi9PXLxZsuxc4D/hHOs5fgN6tvLem9v+woP0HAPsBzwILgDMK6u8IPAQsSnUvAbqnbfen9/JWer/fLjj+j4BXgDFNZWmfj6dzbJ/WNwFeB/Zopb1HpPfTo5XtvwB+kl6vm77/WxdsHwNc0Mq+q/1smm0bCswBVFD2EjCs2v+HvazZUvUG1OICDANWNAWuVuqcCzwM9AE+CvwTOC9t2yPtfy6wVgo4bwO90vbmgbLVwJl+0d8EPpG29QM+nV6vCpzAhsBC4PC038FpfaO0/V7gOWBroCGttxYsmtr/k9T+Y4DXgBuAjwCfBpYBW6b6OwCD03k3B54GTi44XgBbtXD8C8n+ADUUBs5U55h0nHWA8cAv2/hZzAA2Ta8vJAve/wAuSt+PBuC5tH074J1m+/8AuKOVY59N9odoATAdOK5g2/eAu5vVvxM4pdr/h72s2eKuemk2Al6PtrvShwLnRsT8iHiNLJM8vGD78rR9eUTcRZZtlXoNrxHYVlJDRMyLiOkt1PkyMCMixkTEioi4Efg3sH9Bnasj4tmIeAe4BWjretxy4KcRsRy4CegN/CYilqTzTwc+CxARUyLi4XTeF4DfA7sX8Z7Oioh3U3tWExGXkwXER8j+WPy4pYOka6dzI+JlSfsC+wKfA74GDAG6puMvkNQbWI8sEBZaTPYHoSW3AJ8i++N4DPATSQenbXmPZTXCgbM0bwC927n2tgnwYsH6i6ls1TGaBd63yX7RcomIt8i6t8cC8yT9WdIni2hPU5v6F6y/kqM9b0TEyvS6KbC9WrD9nab9JW0t6c40KPMm2bXG3m0cG+C1iFjWTp3LgW2B/4uId1up04esuwzwGeCe9MdsPnBPal8XsmuQC8j+gK3f7Bjrk12++ICI+FdEzI2IlRHxT+A3ZIN75D2W1Q4HztI8RNYVPaCNOnPJBnmafCyVleItsi5pk40LN0bE+Ij4Elnm9W+ygNJee5raNKeFuuV2GVm7BkbE+sAZgNrZp835DiWtR3bd+ErgbEkbtlL1dbLvC8A0YB9JfST1Ibvksi7wc+CuiGgku0bbTdLAgmN8jiyDLkbw/nubDnxWUuF7/WyOY1kn5cBZgohYTHZ971JJB0haR9JakvZNo7cANwJnSvpo6gL+hGx0tRRTgd0kfUzSBsDpTRsk9ZX0VUnrAu+SZTkrWzjGXcDW6RaqbpK+DWxDds2t0j5Cdh12acqGj2u2/VVgyw/s1bbfAFMi4j+BPwO/a6lSRDwLbCqpX0TcTZZlPgGMIxuYOo4sA/xBqv8WcBtwrqR1Je1MdqfEmJaOL2m4pF7K7AicSDaSDtl14pXAiZJ6SDohlf8t53u1zqbaF1lreSG7jjmZLCN8hewX+Itp29rAxWQDEfPS67XTtj0oGOhIZS8Ae6fXZ9NspJbsFplFwEyya2lNg0P9gPvIrp0tIvtl3SbtcySrj6rvAkxJdacAuxRsuxf4z4L11fZt1pbV2p/aEcDmBWUPAoel17uRZZxLgQfIBsUK23Vs+h4tAr7VyvdnVRlZIJsDbJjW10vfl0Nbae/I9LP5wGBeK2UbAn9KP9eXgEMKtu0KLC1Yv5Hs0s3S9B5PbHas7dL3+h3gMWC7av+/9bLmi9IP1+xDTdIlZF3un5BdaulCdjfDhcCQyAatzIriwGl1Q9LXgOPJAihkt4hdGNmgjlnRHDjNzHLy4JCZWU4OnGZmOa3x5AmVsvz1Wb6GUKMaNtm12k2wNbDivTnt3WPbolJ/Z9fqvWVJ56smZ5xmZjl12ozTzGpMY0vPXXw4OXCaWXlEY7Vb0GEcOM2sPBodOM3McglnnGZmOTnjNDPLyRmnmVlOHlU3M8vJGaeZWU6+xmlmlo9H1c3M8nLGaWaWkzNOM7OcPKpuZpaTM04zs5x8jdPMLKc6yjg9kbGZWU7OOM2sPNxVNzPLJ8Kj6mZm+dTRNU4HTjMrD3fVzcxycsZpZpaTnxwyM8vJGaeZWU6+xmlmlpMzTjOznOoo4/Qjl2ZWHo2NpS1FkPSCpGmSpkqanMo2lDRB0oz0tVcql6SLJc2U9KSk7QuOMyLVnyFpREH5Dun4M9O+aqs9DpxmVhYRK0tactgzIgZFxOfT+mnAxIgYCExM6wD7AgPTMhK4DLJAC5wF7ATsCJzVFGxTnZEF+w1rqyEOnGZWHhXMOFsxHBidXo8GDigovzYyDwM9JfUD9gEmRMSCiFgITACGpW3rR8RDERHAtQXHapEDp5mVRzSWtEgaKWlywTKypaMDf5E0pWB734iYB5C+9knl/YGXC/adncraKp/dQnmrPDhkZuVRYvYYEaOAUe1U2zki5krqA0yQ9O826rZ0fTJKKG+VM04zK48SM86iDh0xN32dD9xOdo3y1dTNJn2dn6rPBjYt2H0AMLed8gEtlLfKgdPMOjVJ60r6SNNrYCjwFDAOaBoZHwGMTa/HAUek0fXBwOLUlR8PDJXUKw0KDQXGp21LJA1Oo+lHFByrRe6qm1l5VO4+zr7A7ekOoW7ADRFxj6RJwC2SjgZeAg5M9e8C9gNmAm8DRwFExAJJ5wGTUr1zI2JBen0ccA3QANydllYpG0TqfJa/PqtzNsza1bDJrtVugq2BFe/NafMexta8M/6Skn5nG/Y5oaTzVZMzTjMrjzp6csiB08zKw4HTzCwnT/JhZpaTM04zs5yccZqZ5eSM08wsJ2ecZmY5OeM0M8vJgdPMLKdO+hRiJThwmll5OOM0M8vJgdPMLCePqpuZ5VRHGacnMjYzy8kZp5mVh0fVzcxyqqOuugOnmZWHA6eZWU4eVTczyycafY3TzCwfd9XNzHJyV93MLCd31c3McnJX3cwsJwdOK9bQb4xg3XXWoUuXLnTt2pVbrroYgOtvHcuNf7yDrl27stsXd+SU44/mzvF/4+ob/rhq32efe55br/o/Prn1x1m+fDk//fVvmfT4NLpInDhyBF/acxcmT53Ghb/5Pc8+9zy/OOc0hu65a7Xeat24fNSv+PJ+ezP/tdcZtN0QAHr16smN11/GZpttyosvvsxBhxzLokWL2X//oZxz9qk0NgYrVqzglFPO4h//nFTld1AldfTkkKKTvtnlr8/qnA1rZug3RnDzlRfTq+cGq8oenfIEo669id/+4hy6d+/OGwsXsVGvnqvt9+xzz3Piaedyz61XA3DJFWNobGzkxJEjaGxsZPGbS+jVcwPmzHuVpW+9zTU3/pE9d9mpJgJnwyadv41t2XWXnVi69C2uvvo3qwLnBT//MQsWLOJ/fnEpPzz1eHr12oDTz/gZ6667Dm+99TYAn/nMp7jxht+x7Wd2r2bz19iK9+aolP3e/vUxJf3OrvP9y0s6XzVVLOOU9ElgONAfCGAuMC4inq7UOTuLm//0Z44+7Ft0794d4ANBE+CuCfex797v/4Ld/ue/cMcNlwPQpUuXVYG4f7++WZlq7v9WzXrgwUfYbLMBq5Xtv/8+DNn7mwBcO+ZWJv71D5x+xs9WBU2AdddZh86aiHSIOhocqsjsSJJ+BNwECHgUmJRe3yjptEqcs1okMfJ7P+Zb3/l/3Dr2LgBeeGkOU554ioOPOZkjjz+VaU8/84H97pl4H/t9aQ8A3lyyFIBLLr+WA486ge+f+VNeX7Cww96Dta9vn9688sp8AF55ZT59PrrRqm3Dhw/jqWn3MW7saI455pRqNbH6orG0pQZVKuM8Gvh0RCwvLJT0a2A6cEGFztvhxlz2K/p8dCPeWLiIY04+gy0225SVK1fy5pKl3DDqIp56+ll+8N8/555br0Ypa3xy+r9pWHttBm65OQArV67k1fmvs91ntuGHJ45k9E238ctLruCCn5xaxXdmxRo79h7Gjr2HXXfZiXPOPpV99j2o2k2qDmeca6wR2KSF8n5pW4skjZQ0WdLkK669sUJNK6+mzGOjXj0ZstsXmfavZ+jbpzd7774zkvjMNp9AEgsXLV61z91/Xb2b3nOD9WlYuwdDdv8iAEP33JWnn5nZsW/E2vTq/NfZeOM+AGy8cR/mv/bGB+o88OAjbLnlZmy0Ua+Obl6nEI2NJS21qFKB82RgoqS7JY1Kyz3AROCk1naKiFER8fmI+Px/HnFwhZpWPm+/s2zVNa6331nGPx99jIFbbs5eu36BR6dMBeCFl2azfMWKVdcsGxsb+cvfH1gtcEpi9513YtLjTwLwyOSpfHyLj3Xwu7G23HnHXzji8AMBOOLwA7njjvEAfPzjm6+qs92gbenefS3eeMOXWT7sKtJVj4h7JG0N7Eg2OCRgNjApIlZW4pzV8MaChZx0xnkArFyxkv2G7sEugz/P8uXLOfNnF3HAYcey1lrd+NmZp6zqpk+e+hR9P9qbTfv3W+1Y3/+v73D6ub/kgt/8ng17bsD5Z3wfgGlPP8PJp5/Hm0uWcu8/HuHSK65j7PW/79g3WmeuG3Mpu+/2BXr33pAXZk3mnHN/yYW/uJSbbvgdRx15MC+/PIdvH/xdAL7+tf047LBvsnz5Cpa9s4xDDj2uyq2vojrqqvt2JCu7Wr8dqd6VejvSW+cfVtLv7LpnXldzt4z4BngzK486yjgdOM2sPGp0oKcUDpxmVh7OOM3McqrRm9lL4c9VN7PyaIzSliJI6irpcUl3pvUtJD0iaYakmyV1T+U90vrMtH3zgmOcnsqfkbRPQfmwVDaz2CcbHTjNrCwqfAP8SUDhPBcXAhdFxEBgIdnTiqSvCyNiK+CiVA9J2wAHAZ8GhgG/TcG4K3ApsC+wDXBwqtsmB04zK48KZZySBgBfBq5I6wL2Av6QqowGDkivh6d10vYhqf5w4KaIeDcingdmkt1nviMwMyJmRcR7ZHNsDG+vTQ6cZlYeleuq/y/wQ95/XHsjYFFErEjrs8ketCF9fRkgbV+c6q8qb7ZPa+VtcuA0s/IocXakwjkq0jKy6ZCSvgLMj4gpBWdq6Yb5aGdb3vI2eVTdzMqjxNuRImIUMKqVzTsDX5W0H7A2sD5ZBtpTUreUVQ4gm+8XsoxxU2C2pG7ABsCCgvImhfu0Vt4qZ5xmVhbRGCUtbR4z4vSIGBARm5MN7vwtIg4F/g58M1UbAYxNr8elddL2v0X2XPk44KA06r4FMJD35woemEbpu6dzjGvvvTrjNLPy6Ngb4H8E3CTpfOBx4MpUfiUwRtJMskzzIICImC7pFuBfwArg+KYJhySdAIwHugJXRcT09k7uST6s7DzJR20rdZKPJSfsV9Lv7EcuucuTfJhZnfIjl2ZmOdVR4PTgkJlZTs44zawsOut4SSU4cJpZedRRV92B08zKw4HTzCyf9m5m/zBx4DSz8nDgNDPLqX4mgHfgNLPycFfdzCwvB04zs5zcVTczy8dddTOzvJxxmpnl44zTzCwvZ5xmZvmEA6eZWU4OnGZm+dRTxumJjM3McnLGaWblUUcZpwOnmZVFPXXVHTjNrCwcOAFJG7a1Y0QsKH9zzKxWOXBmpgABtPRh8QFsWZEWmVltipZCxYdTq4EzIrboyIaYWW2rp4yz3duRlDlM0n+n9Y9J2rHyTTOzWhKNKmmpRcXcx/lb4AvAIWl9CXBpxVpkZjUpGktbalExo+o7RcT2kh4HiIiFkrpXuF1mVmPC1zhXs1xSV7IBISR9lLq61dXMilGr2WMpigmcFwO3A30l/RT4JnBmRVtlZjWnVq9XlqLdwBkR10uaAgxJRQdExNOVbZaZ1Zqon3mMi35yaB2gqbveULnmmFmtqqeMs5jbkX4CjAY2BHoDV0tyV93MVlNPtyMVk3EeDGwXEcsAJF0APAacX8mGmVltcVd9dS8AawPL0noP4LlKNcjMalOtZo+laGuSj/8ju6b5LjBd0oS0/iXgwY5pnplZ59NWxjk5fZ1CdjtSk3sr1hozq1m+AR6IiNEd2RAzq231dAN8MaPqAyX9QdK/JM1qWjqicWZWOxpDJS3tkbS2pEclPSFpuqRzUvkWkh6RNEPSzU2PgkvqkdZnpu2bFxzr9FT+jKR9CsqHpbKZkk5rr03FTPJxNXAZsALYE7gWGFPEfmZWRyJU0lKEd4G9IuJzwCBgmKTBwIXARRExEFgIHJ3qHw0sjIitgItSPSRtAxwEfBoYBvxWUtf0SPmlwL7ANsDBqW6rigmcDRExEVBEvBgRZwN7FfNuzax+VOo+zsgsTatrpSXI4tAfUvlo4ID0enhaJ20fIkmp/KaIeDcingdmAjumZWZEzIqI94CbUt1WFRM4l0nqAsyQdIKkrwF9itjPzOpIRGlLMVJmOBWYD0wguyVyUUSsSFVmA/3T6/7Ay1mbYgWwGNiosLzZPq2Vt6qYwHky2SOXJwI7AIcDI4rYz8zqSKkZp6SRkiYXLCM/cOyIlRExCBhAliF+qqUmpK+tfdxP3vJWFTPJx6T0cilwVHv1zaw+FTPQ05KIGAWMKrLuIkn3AoOBnpK6paxyADA3VZsNbArMltQN2ABYUFDepHCf1spb1NYN8HfQRtSNiK+2dWAzqy+Vuo8zzQG8PAXNBmBvsgGfv5NNc3kTWS94bNplXFp/KG3/W0SEpHHADZJ+DWwCDAQeJcs4B0raAphDNoDU9IkXLWor4/xlSe/SzOpSBZ9V7weMTqPfXYBbIuJOSf8CbpJ0PvA4cGWqfyUwRtJMskzzoKx9MV3SLcC/yO4SOj4iVgJIOgEYTzYL3FURMb2tBik66ZP5y1+f1TkbZu1q2GTXajfB1sCK9+aUlDpO3eyrJf3ODnpxXM09clTsfJxmZm3yI5dmZjl10s5rRXTawOnunlltKXVUvRZ5VN3MysJd9YxH1c2saM44gYi4ryMbYmZWK9q9xilpIPBzsllD1m4qj4gtK9guM6sxdTQ2VNTg0NXAWWTTM+1J9thl/eTkZlaUeuqqe1o5MyuLCs7H2ekUk3GuNq0c2bOcnlbOzFZTR5+c4WnlzKw8ApW01CJPK2dmZdFYR6NDxYyq/50WBswiwtc5zWyVxhrNHktRzDXOHxS8Xhv4BtmUTGZmq9Rqt7sUxXTVpzQr+ock3xxvZqupp8GhYrrqGxasdiEbINq4Yi0ys5rkjHN1U3j/A41WAM/z/ucXm5kBzjib+1RELCsskNSjQu0xsxpVT4GzmPs4/9lC2UPlboiZ1TbfxwlI2pjsQ9kbJG3H+8+nr092Q7yZ2SqNtRkDS9JWV30f4Eiyzxj+Fe8HzjeBMyrbLDOrNb6PE4iI0WQfyfmNiPhjB7bJzGpQHT04VNQ1zh0k9WxakdQrfY6xmVldKiZw7hsRi5pWImIhsF/lmmRmtaixxKUWFXM7UldJPSLiXQBJDYBvRzKz1TTK1zgLXQdMlHQ12WWM7wDXVrRVZlZz6ukaZzHPqv+PpCeBvclG1s+LiPEVb5mZ1ZRa7XaXopiMk4i4B7gHQNLOki6NiOMr2jIzqym+j7MZSYOAg4Fvkz2rflslG2Vmtcf3cQKStgYOIguYbwA3k31g254d1DYzqyG+xpn5N/AAsH9EzASQ9L0OaZWZ1Zx66qq3dR/nN4BXgL9LulzSEPx56mbWinq6j7PVwBkRt0fEt4FPAvcC3wP6SrpM0tAOap+Z1YgocalF7T45FBFvRcT1EfEVsgk/pgKnVbxlZlZTGlXaUouKeeRylYhYEBG/9ydcmllz9dRVL+p2JDOz9tRqECyFA6eZlUXUaLe7FA6cZlYWzjjNzHKqp8CZa3DIzKw1lbodSdKmkv4u6WlJ0yWdlMo3lDRB0oz0tVcql6SLJc2U9KSk7QuONSLVnyFpREH5DpKmpX0ultqeI8+B08w6uxXAKRHxKWAwcLykbchui5wYEQOBibx/m+S+wMC0jAQugyzQAmcBOwE7Amc1BdtUZ2TBfsPaapADp5mVRaXu44yIeRHxWHq9BHia7BN4hwOjU7XRwAHp9XDg2sg8DPSU1I/sAygnpNsqFwITgGFp2/oR8VBEBNl8w03HapGvcZpZWXTENU5JmwPbAY8AfSNiHmTBVVKfVK0/8HLBbrNTWVvls1sob5UzTjMri1JvgJc0UtLkgmVkS8eXtB7wR+DkiHizjaa0lMdGCeWtcsZpZmVR6nPnETEKGNVWHUlrkQXN6yOiaT7gVyX1S9lmP2B+Kp8NbFqw+wBgbirfo1n5val8QAv1W+WM08zKolLXONMI95XA0xHx64JN44CmkfERwNiC8iPS6PpgYHHq0o8HhqaPOO8FDAXGp21LJA1O5zqi4FgtcsZpZmVRwWucOwOHA9MkTU1lZwAXALdIOhp4CTgwbbuL7CPMZwJvA0dBNteGpPOASaneuRGxIL0+DrgGaADuTkurHDjNrCwqNUVcRDxI63MBD2mhfgAtfiZaRFwFXNVC+WRg22Lb5MBpZmXRWLOza+bnwGlmZVFPj1w6cJpZWdRPvunAaWZl4ozTzCynWv0YjFI4cJpZWXhwyMwsp/oJmw6cZlYmvsZpZpZTPXXV/ay6mVlOzjjNrCzqJ9904DSzMvE1TjOznOrpGqcDp5mVRf2ETQdOMysTd9XNzHKKOso5HTjNrCyccZqZ5VRPg0O+Ab4DXD7qV8yd/QRTH5+4quzCn5/JU9Pu47EpE/jDrVewwQbrV7GF1paTTjyGJ6b+jamPT+S6MZfSo0cPAM4790f8a/oDTHvyXk44/jtVbmX1RYlLLXLg7ADXXnsLX/7KoauV/XXi/Xxu0F5sv8OXmDFjFqf96IQqtc7asskmG3PC8d9hp8H7MWi7IXTt2pVvf2s4I474FgMGbMKnt92Nz3x2D26+pc0PRawLjURJSy1y4OwADzz4CAsWLlqtbMJf72flypUAPPzIY/Tv368aTbMidOvWjYaGtenatSvrNDQwb94rHPvdIzj/pxeRfS4YvPbaG1VuZfU1lrjUog4PnJKO6uhzdnZHHXkQ94z/e7WbYS2YO/cVfn3R73j+uUeZ/dLjLH7zTSb89X623HJzvnXgV3n4obu4c9wYttpqi2o3teqixH+1qBoZ5zlVOGendfppJ7JixQpuuOG2ajfFWtCz5wZ8df992GrrwWy62fasu+46HHLI1+nRozvLlr3L4C/sxxVX3cAVo35V7aZWXT1lnBUZVZf0ZGubgL5t7DcSGAmgrhvQpcu6FWhd53H44Qfy5f325kv7fKvaTbFWDBmyK8+/8BKvv74AgNv/dDdfGPx5Zs+Zx223/xmAP/3pbq68/NfVbGanUKvZYykqdTtSX2AfYGGzcgH/bG2niBgFjALo1r3/h/qnsM/QPTj1B//FXkO+wTvvLKt2c6wVL780h5122p6GhrV5551l7LXnLkyZ8gRLlixhzz125prRN7P7bl/g2Rmzqt3UqqvV7LEUlQqcdwLrRcTU5hsk3Vuhc3Za1425lN13+wK9e2/IC7Mmc865v+RHPzyBHj16cM/dNwHwyCOPcfwJp1W5pdbco5Me57bb/sykR8ezYsUKpk6dzuVXXE9Dw9qMGX0JJ510DG8tfZvvHntqtZtadY3xoc51VqPopG/2w55xmnVWK96bU9LnVR6+2ddL+p0d8+JtNff5mH5yyMzKop4yHQdOMyuLWr2ZvRQOnGZWFh5VNzPLyaPqZmY5uatuZpaTu+pmZjm5q25mllNnvSe8Ehw4zawsfI3TzCwnd9XNzHLy4JCZWU711FX3R2eYWVlERElLeyRdJWm+pKcKyjaUNEHSjPS1VyqXpIslzZT0pKTtC/YZkerPkDSioHwHSdPSPhdLanfSEQdOMyuLCs4Afw0wrFnZacDEiBgITEzrAPsCA9MyErgMskALnAXsBOwInNUUbFOdkQX7NT/XBzhwmllZVOozhyLifmBBs+LhwOj0ejRwQEH5tZF5GOgpqR/ZxOoTImJBRCwEJgDD0rb1I+KhyNLfawuO1Spf4zSzsujga5x9I2IeQETMk9QnlfcHXi6oNzuVtVU+u4XyNjnjNLOqkjRS0uSCZeSaHK6FsiihvE3OOM2sLEp9cqjws8ZyeFVSv5Rt9gPmp/LZwKYF9QYAc1P5Hs3K703lA1qo3yZnnGZWFo1ESUuJxgFNI+MjgLEF5Uek0fXBwOLUpR8PDJXUKw0KDQXGp21LJA1Oo+lHFByrVc44zawsKnUDvKQbybLF3pJmk42OXwDcIulo4CXgwFT9LmA/YCbwNnAUQEQskHQeMCnVOzcimgacjiMbuW8A7k5L223qrA/m+8PazKqj1A9r263/kJJ+Z++fM9Ef1mZm9ameMh0HTjMri3p65NKB08zKwoHTzCynzjpeUgkOnGZWFs44zcxy8nycZmY5uatuZpaTu+pmZjk54zQzy8kZp5lZTh4cMjPLqbGOuuqeVs7MLOknd2sAAATzSURBVCdnnGZWFu6qm5nlVE9ddQdOMysLZ5xmZjk54zQzy8kZp5lZTs44zcxycsZpZpZTRGO1m9BhHDjNrCz8rLqZWU6eHcnMLCdnnGZmOTnjNDPLybcjmZnl5NuRzMxyclfdzCwnDw6ZmeVUTxmnZ4A3M8vJGaeZlYVH1c3McqqnrroDp5mVhQeHzMxycsZpZpaTr3GameXkJ4fMzHJyxmlmlpOvcZqZ5eSuuplZTs44zcxycuA0M8upfsImqJ7+SnQmkkZGxKhqt8NK459fffPsSNUzstoNsDXin18dc+A0M8vJgdPMLCcHzurx9bHa5p9fHfPgkJlZTs44zcxycuCsAknDJD0jaaak06rdHiuepKskzZf0VLXbYtXjwNnBJHUFLgX2BbYBDpa0TXVbZTlcAwyrdiOsuhw4O96OwMyImBUR7wE3AcOr3CYrUkTcDyyodjusuhw4O15/4OWC9dmpzMxqhANnx1MLZb61wayGOHB2vNnApgXrA4C5VWqLmZXAgbPjTQIGStpCUnfgIGBcldtkZjk4cHawiFgBnACMB54GbomI6dVtlRVL0o3AQ8AnJM2WdHS122Qdz08OmZnl5IzTzCwnB04zs5wcOM3McnLgNDPLyYHTzCwnB84PAUkrJU2V9JSkWyWtswbH2kPSnen1V9uavUlST0n/VcI5zpb0g2LLm9W5RtI3c5xrc89kZOXmwPnh8E5EDIqIbYH3gGMLNyqT+2cdEeMi4oI2qvQEcgdOs1rnwPnh8wCwVcq0npb0W+AxYFNJQyU9JOmxlJmuB6vmB/23pAeBrzcdSNKRki5Jr/tKul3SE2n5InAB8PGU7f4i1TtV0iRJT0o6p+BYP05zkP4V+ER7b0LSMek4T0j6Y7Msem9JD0h6VtJXUv2ukn5RcO7vruk30qw1DpwfIpK6kc3zOS0VfQK4NiK2A94CzgT2jojtgcnA9yWtDVwO7A/sCmzcyuEvBu6LiM8B2wPTgdOA51K2e6qkocBAsqnzBgE7SNpN0g5kj5ZuRxaY/6OIt3NbRPxHOt/TQOETOpsDuwNfBn6X3sPRwOKI+I90/GMkbVHEecxy61btBlhZNEiaml4/AFwJbAK8GBEPp/LBZBMn/0MSQHeyRwc/CTwfETMAJF1Hy58ZvhdwBEBErAQWS+rVrM7QtDye1tcjC6QfAW6PiLfTOYp5Nn9bSeeTXQ5Yj+wR1Sa3REQjMEPSrPQehgKfLbj+uUE697NFnMssFwfOD4d3ImJQYUEKjm8VFgETIuLgZvUGUb5p7QT8PCJ+3+wcJ5dwjmuAAyLiCUlHAnsUbGt+rEjn/n8RURhgkbR5zvOatctd9frxMLCzpK0AJK0jaWvg38AWkj6e6h3cyv4TgePSvl0lrQ8sIcsmm4wHvlNw7bS/pD7A/cDXJDVI+gjZZYH2fASYJ2kt4NBm2w6U1CW1eUvgmXTu41J9JG0tad0izmOWmzPOOhERr6XM7UZJPVLxmRHxrKSRwJ8lvQ48CGzbwiFOAkal2YBWAsdFxEOS/pFu97k7Xef8FPBQyniXAodFxGOSbgamAi+SXU5oz38Dj6T601g9QD8D3Af0BY6NiGWSriC79vmYspO/BhxQ3HfHLB/PjmRmlpO76mZmOTlwmpnl5MBpZpaTA6eZWU4OnGZmOTlwmpnl5MBpZpaTA6eZWU7/H4UoCXAZv47OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_results = model3.evaluate(X_test,Y_test,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model3.metrics_names, sampled_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "\n",
    "plot_cm(Y_test,test_predictions_w  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
